@article{HUNG2025103005,
title = {Factors driving user behavior and value creation with text-to-image generative artificial intelligence (AI): A systems theory perspective},
journal = {Technology in Society},
volume = {83},
pages = {103005},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.103005},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25001952},
author = {Chih-Lung Hung and Jen-Her Wu and Po-Chuen Chiang and Qi Li and Yi-Cheng Chen},
keywords = {Text-to-image generative AI, Systems theory, Compatibility, Synergy, User value},
abstract = {The rise of image-generative AI has made it a crucial tool for image creators, gaining popularity in fields such as cartography, design, and photography. As users increasingly rely on AI for image creation, understanding the factors that drive user value becomes essential. Drawing on systems theory, this study proposes a conceptual framework to examine the relationships among integration effort, compatibility, synergy, and value creation. Data from 531 AI image creators supported our hypotheses, revealing that: (1) integration effort significantly enhances both compatibility and synergy; (2) compatibility positively influences synergy; (3) synergy directly and positively impacts user value creation; and (4) synergy serves as a critical mediator in the relationships between the two enablers—compatibility and integration effort—and user value creation, with compatibility also partially mediating the link between integration effort and synergy. These results extend systems theory by integrating it more deeply with the resource-based view and highlighting synergy as a pivotal factor that not only directly drives user value creation but also mediates the effects of compatibility and integration effort. Furthermore, the study empirically confirms that user value can be conceptualized through three key constructs: efficiency, effectiveness, and innovation.}
}
@article{WHITE2025452,
title = {Generative artificial intelligence tools in journal article preparation: A preliminary catalog of ethical considerations, opportunities, and pitfalls*},
journal = {JDS Communications},
volume = {6},
number = {3},
pages = {452-457},
year = {2025},
issn = {2666-9102},
doi = {https://doi.org/10.3168/jdsc.2024-0707},
url = {https://www.sciencedirect.com/science/article/pii/S2666910224002011},
author = {Robin R. White},
abstract = {The launch of generative artificial intelligence (GenAI) tools has catalyzed considerable discussion about the potential impacts of these systems within the scientific article preparation process. This symposium paper seeks to summarize current recommendations on the use of GenAI tools in scientific article preparation, and to provide speculations about the future challenges and opportunities of GenAI use in scientific publishing. Due to the dynamic nature of these tools and the rapid advancement of their sophistication, the most important recommendation is that ongoing engagement and discussion within the scientific community about these issues is critical. When using GenAI tools in scientific article preparation, humans are ultimately accountable and responsible for products produced. Given that accountability, an expert panel convened by the National Academies of Science, Engineering, and Medicine recently proposed principles of GenAI use in science communication, including (1) transparent disclosure and attribution; (2) verification of AI-generated content and analyses; (3) documentation of artificial intelligence (AI)-generated data; (4) a focus on ethics and equity; and (5) continuous monitoring, oversight, and public engagement. In addition to the importance of human accountability, many publishers have established consistent policies suggesting that GenAI tools should not be used for peer reviewing, figure generation or manipulation, or assigned authorship on scientific articles. Along with the potential ethical challenges associated with GenAI use in scientific publishing, there are numerous potential benefits. Herein we summarize example conversations demonstrating the capacity of GenAI tools to support the article preparation process, and an example standard operating procedure for human-AI interaction in article preparation. Finally, diverse broader questions about the impact of GenAI tools on communication, knowledge, and advancement of science are raised for rumination.}
}
@article{RAHMAN20243,
title = {Generative artificial intelligence: opportunities, challenges and future avenues for organizational learning},
journal = {Development and Learning in Organizations: An International Journal},
volume = {39},
number = {2},
pages = {3-7},
year = {2024},
issn = {1477-7282},
doi = {https://doi.org/10.1108/DLO-04-2024-0101},
url = {https://www.sciencedirect.com/science/article/pii/S1477728224000534},
author = {Hanfia Rahman and Tripti Singh},
keywords = {Generative artificial intelligence, Organizational learning, Systematic literature review},
abstract = {Purpose
This study provides a comprehensive investigation of the opportunities and challenges associated with generative artificial intelligence (GAI) use in development and learning in organizations. Additionally, it highlights the future avenues in GAI research and provides practical recommendations for policymakers.
Design/methodology/approach
Data for the review was collected from the Web of Science database using the search criteria (“Generative artificial intelligence” OR “artificial intelligence”) AND (“human resource management” OR “human resource development” OR “workplace learning” OR “organizational learning” OR “organizational development” OR “learning organization”) on March 6th, 2024. Filtering results to Management and Business categories yielded 71 articles. After abstract review, 6 unrelated articles were excluded, leaving 65 articles for final analysis.
Findings
The study presents several opportunities of GAI such as applications in personal learning and content generation. Moreover, it unravels several potential challenges of GAI such as quality and accuracy issues and elucidates several future research directions.
Originality/value
Our study is the first literature review that provides and a comprehensive overview of generative artificial intelligence in the context of organizational learning.}
}
@article{SIMMS2025106544,
title = {Generative artificial intelligence (AI) literacy in nursing education: A crucial call to action},
journal = {Nurse Education Today},
volume = {146},
pages = {106544},
year = {2025},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2024.106544},
url = {https://www.sciencedirect.com/science/article/pii/S0260691724004544},
author = {Rachel C. Simms},
keywords = {Artificial intelligence, Nursing education, Educational technology, Ethics, Nursing},
abstract = {Introduction
Generative artificial intelligence (AI) is revolutionizing healthcare, necessitating corresponding advancements in nursing education to ensure that future nurses are equipped for a technologically driven environment. This article explores the imperative integration of generative AI literacy in nursing education.
Implications for nurse educators
The article delves into the practical challenges and opportunities presented by generative AI in nursing. It underscores the need for educators to adapt curricula and teaching methods to effectively incorporate generative AI learning, ensuring students are proficient in generative AI technologies and aware of their ethical implications.
Generative AI literacy
Defined as a core educational requirement, this section highlights the skills and knowledge that nurse educators must impart. It encompasses the ability to critically assess AI-generated content, understand the underlying technologies, and responsibly apply this knowledge in clinical settings.
Conclusion
The article concludes by emphasizing the urgency of integrating generative AI literacy into nursing education. It advocates for a proactive approach to curriculum development and calls for global collaboration and standardization in AI education to address the diverse and evolving needs of healthcare.}
}
@article{SUN2025103806,
title = {Enhancing critical language awareness in EAL writing education amid the rise of generative artificial intelligence},
journal = {System},
volume = {134},
pages = {103806},
year = {2025},
issn = {0346-251X},
doi = {https://doi.org/10.1016/j.system.2025.103806},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X25002167},
author = {Yachao Sun and Ge Lan},
keywords = {Generative artificial intelligence, Critical language awareness, English as an additional language, Writing education, Critical AI literacy, Digital literacies},
abstract = {The emergence of generative artificial intelligence (GenAI) tools has presented both opportunities and challenges in English as an Additional Language (EAL) writing education. This mixed-methods exploratory study investigated how Chinese EAL students perceived and used GenAI, and how these attitudes and behaviors reflected their Critical Language Awareness (CLA). Survey data were collected from 320 Chinese undergraduate and graduate students studying in China and abroad, supplemented by in-depth semi-structured interviews with five volunteers. Quantitative findings revealed that, while participants valued GenAI for reducing language barriers and enhancing writing mechanics, they remained wary of issues related to originality, transparency, and ethical accountability. Qualitative analysis further illuminated participants' emerging CLA, evident in their negotiations of power (managing overreliance and misinformation), ideology (navigating dominant academic norms), and social justice (acknowledging cultural biases encoded in GenAI training data). Although students were primarily motivated by pragmatic benefits, such as improved grammar, structure, and efficiency, they also recognized GenAI's potential to perpetuate monolingual, Western-centric perspectives. These results underscore the need to embed CLA principles in GenAI-assisted pedagogy. By encouraging students to critically interrogate GenAI outputs, question linguistic hierarchies, and reflect on sociocultural contexts, educators can better leverage GenAI's affordances while fostering ethical, culturally responsive, and equitable writing practices.}
}
@article{VISSAK2025436,
title = {Applying generative artificial intelligence applications for academic research on firms’ nonlinear internationalization},
journal = {Review of International Business and Strategy},
volume = {35},
number = {4},
pages = {436-484},
year = {2025},
issn = {2059-6014},
doi = {https://doi.org/10.1108/RIBS-10-2024-0120},
url = {https://www.sciencedirect.com/science/article/pii/S2059601425000037},
author = {Tiia Vissak and Lasse Torkkeli},
keywords = {Nonlinear internationalization, De-internationalization, Re-internationalization, Internationalization, Generative artificial intelligence (GenAI) tools, GenAI tools in research},
abstract = {Purpose
This study aims to critically evaluate the applicability of generative artificial intelligence (GenAI) tools for academic research in international business (IB), specifically focusing on the topic of firms’ nonlinear internationalization. It assesses these tools’ key performance dimensions: correctness, hallucinations and thoroughness.
Design/methodology/approach
This research adopts an exploratory approach, examining a comprehensive set of GenAI tools: eight chatbots and four AI-driven applications designed for academic purposes. The evaluation focuses on the capabilities and limitations of these tools in generating accurate research-related content for IB scholars.
Findings
This study finds that while GenAI tools capture some aspects of nonlinear internationalization, they often produce partially accurate and/or biased results. Common issues include providing fictitious sources, incorrect publication data and vague or incorrect answers. Thus, substantial development is still needed for GenAI tools to become reliable for scientific research.
Practical implications
Researchers should use GenAI tools with caution, verifying the accuracy of generated content and citations independently. A cautious approach is crucial to maintain the integrity and quality of academic research.
Social implications
This study raises awareness about ethical and practical challenges of using AI in academia, including issues related to plagiarism and misinformation. It underscores the importance of critical evaluation when using GenAI tools for research.
Originality/value
This paper contributes to the emerging literature on the role of GenAI in academic research by providing a critical assessment of the usability and limitations of current tools in studying complex IB phenomena. By using nonlinear internationalization as an example, it demonstrates how GenAI may support or hinder IB scholarship.}
}
@article{LUO2025,
title = {Generative Artificial Intelligence Tools in Medical Research (GAMER): Protocol for a Scoping Review and Development of Reporting Guidelines},
journal = {JMIR Research Protocols},
volume = {14},
year = {2025},
issn = {1929-0748},
doi = {https://doi.org/10.2196/64640},
url = {https://www.sciencedirect.com/science/article/pii/S1929074825005001},
author = {Xufei Luo and Yih Chung Tham and Mohammad Daher and Zhaoxiang Bian and Yaolong Chen and Janne Estill},
keywords = {generative AI, chatbots, reporting guidelines, transparency, Delphi method, large language models, ChatGPT},
abstract = {Background
The integration of artificial intelligence (AI) has revolutionized medical research, offering innovative solutions for data collection, patient engagement, and information dissemination. Powerful generative AI (GenAI) tools and other similar chatbots have emerged, facilitating user interactions with virtual conversational agents. However, the increasing use of GenAI tools in medical research presents challenges, including ethical concerns, data privacy issues, and the potential for generating false content. These issues necessitate standardization of reporting to ensure transparency and scientific rigor.
Objective
The development of the Generative Artificial Intelligence Tools in Medical Research (GAMER) reporting guidelines aims to establish comprehensive, standardized guidelines for reporting the use of GenAI tools in medical research.
Methods
The GAMER guidelines are being developed following the methodology recommended by the Enhancing the Quality and Transparency of Health Research (EQUATOR) Network, involving a scoping review and expert Delphi consensus. The scoping review searched PubMed, Web of Science, Embase, CINAHL, PsycINFO, and Google Scholar (for the first 200 results) using keywords like “generative AI” and “medical research” to identify reporting elements in GenAI-related studies. The Delphi process involves 30-50 experts with ≥3 years of experience in AI applications or medical research, selected based on publication records and expertise across disciplines (eg, clinicians and data scientists) and regions (eg, Asia and Europe). A 7-point-scale survey will establish consensus on checklist items. The testing phase invites authors to apply the GAMER checklist to GenAI-related manuscripts and provide feedback via a questionnaire, while experts assess reliability (κ statistic) and usability (time taken, 7-point Likert scale). The study has been approved by the Ethics Committee of the Institute of Health Data Science at Lanzhou University (HDS-202406-01).
Results
The GAMER project was launched in July 2023 by the Evidence-Based Medicine Center of Lanzhou University and the WHO Collaborating Centre for Guideline Implementation and Knowledge Translation, and it concluded in July 2024. The scoping review was completed in November 2023. The Delphi process was conducted from October 2023 to April 2024. The testing phase began in March 2025 and is ongoing. The expected outcome of the GAMER project is a reporting checklist accompanied by relevant terminology, examples, and explanations to guide stakeholders in better reporting the use of GenAI tools.
Conclusions
GAMER aims to guide researchers, reviewers, and editors in the transparent and scientific application of GenAI tools in medical research. By providing a standardized reporting checklist, GAMER seeks to enhance the clarity, completeness, and integrity of research involving GenAI tools, thereby promoting collaboration, comparability, and cumulative knowledge generation in AI-driven health care technologies.
International Registered Report Identifier (IRRID)
DERR1-10.2196/64640}
}
@article{HASAN2025874,
title = {Ethical Application of Generative Artificial Intelligence in Medicine},
journal = {Arthroscopy: The Journal of Arthroscopic & Related Surgery},
volume = {41},
number = {4},
pages = {874-885},
year = {2025},
issn = {0749-8063},
doi = {https://doi.org/10.1016/j.arthro.2024.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S074980632401048X},
author = {Sayyida S. Hasan and Matthew S. Fury and Joshua J. Woo and Kyle N. Kunze and Prem N. Ramkumar},
abstract = {Generative artificial intelligence (AI) may revolutionize health care, providing solutions that range from enhancing diagnostic accuracy to personalizing treatment plans. However, its rapid and largely unregulated integration into medicine raises ethical concerns related to data integrity, patient safety, and appropriate oversight. One of the primary ethical challenges lies in generative AI’s potential to produce misleading or fabricated information, posing risks of misdiagnosis or inappropriate treatment recommendations, which underscore the necessity for robust physician oversight. Transparency also remains a critical concern, as the closed-source nature of many large-language models prevents both patients and health care providers from understanding the reasoning behind AI-generated outputs, potentially eroding trust. The lack of regulatory approval for AI as a medical device, combined with concerns around the security of patient-derived data and AI-generated synthetic data, further complicates its safe integration into clinical workflows. Furthermore, synthetic datasets generated by AI, although valuable for augmenting research in areas with scarce data, complicate questions of data ownership, patient consent, and scientific validity. In addition, generative AI’s ability to streamline administrative tasks risks depersonalizing care, further distancing providers from patients. These challenges compound the deeper issues plaguing the health care system, including the emphasis of volume and speed over value and expertise. The use of generative AI in medicine brings about mass scaling of synthetic information, thereby necessitating careful adoption to protect patient care and medical advancement. Given these considerations, generative AI applications warrant regulatory and critical scrutiny. Key starting points include establishing strict standards for data security and transparency, implementing oversight akin to institutional review boards to govern data usage, and developing interdisciplinary guidelines that involve developers, clinicians, and ethicists. By addressing these concerns, we can better align generative AI adoption with the core foundations of humanistic health care, preserving patient safety, autonomy, and trust while harnessing AI’s transformative potential.
Level of Evidence
Level V, expert opinion.}
}
@article{KUMAR2025115160,
title = {Generative artificial intelligence (GenAI) revolution: A deep dive into GenAI adoption},
journal = {Journal of Business Research},
volume = {189},
pages = {115160},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115160},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324006647},
author = {Aman Kumar and Amit Shankar and Linda D. Hollebeek and Abhishek Behl and Weng Marc Lim},
keywords = {Artificial intelligence, Generative artificial intelligence, Generative AI, GenAI, Adoption, Behavioral reasoning theory, Mixed methods},
abstract = {This study examines key reasons (for and against) that influence business-to-business (B2B) managers’ intention to adopt generative artificial intelligence (GenAI). We also investigate how GenAI adoption influences firm performance, along with the moderating effect of ethical leadership. Study 1 undertakes a series of in-depth interviews, yielding a set of hypotheses that are tested in Study 2. A total of 277 responses was collected from respondents in the USA, the UK, Canada, India, Australia, Malaysia, and Japan to test the proposed model using structural equation modeling. The findings highlight that need for uniqueness, information completeness, convenience, and deceptiveness significantly impact GenAI adoption. The results also highlight that GenAI adoption boosts firm performance. Finally, ethical leadership was found to moderate the effect of GenAI adoption on firm performance. This study enriches the GenAI, technology adoption, and behavioral reasoning theory literatures while also providing pertinent insights for firms intending to adopt GenAI.}
}
@article{SCHULZEBALHORN2025109121,
title = {Graph-to-SFILES: Control structure prediction from process topologies using generative artificial intelligence},
journal = {Computers & Chemical Engineering},
volume = {199},
pages = {109121},
year = {2025},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2025.109121},
url = {https://www.sciencedirect.com/science/article/pii/S0098135425001255},
author = {Lukas {Schulze Balhorn} and Kevin Degens and Artur M. Schweidtmann},
keywords = {Control structure prediction, Graph-to-sequence, Process graph, SFILES 2.0, Generative artificial intelligence},
abstract = {Control structure design is an important but tedious step in P&ID development. Generative artificial intelligence (AI) promises to reduce P&ID development time by supporting engineers. Previous research on generative AI in chemical process design mainly represented processes by sequences. However, graphs offer a promising alternative because of their permutation invariance. We propose the Graph-to-SFILES model, a generative AI method to predict control structures from flowsheet topologies. The Graph-to-SFILES model takes the flowsheet topology as a graph input and returns a control-extended flowsheet as a sequence in the SFILES 2.0 notation. We compare four different graph encoder architectures, one of them being a graph neural network (GNN) proposed in this work. The Graph-to-SFILES model achieves a top-5 accuracy of 73.2% when trained on 10,000 flowsheet topologies. In addition, the proposed GNN performs best among the encoder architectures. Compared to a purely sequence-based approach, the Graph-to-SFILES model improves the top-5 accuracy for a relatively small training dataset of 1,000 flowsheets from 0.9% to 28.4%. However, the sequence-based approach performs better on a large-scale dataset of 100,000 flowsheets. These results highlight the potential of graph-based AI models to accelerate P&ID development in small-data regimes but their effectiveness on industry relevant case studies still needs to be investigated.}
}
@article{YIN2025100227,
title = {A systematic examination of generative artificial intelligence (GenAI) use guidelines in applied linguistics journals},
journal = {Research Methods in Applied Linguistics},
volume = {4},
number = {3},
pages = {100227},
year = {2025},
issn = {2772-7661},
doi = {https://doi.org/10.1016/j.rmal.2025.100227},
url = {https://www.sciencedirect.com/science/article/pii/S2772766125000485},
author = {Shuhui Yin and Carol A. Chapelle},
keywords = {GenAI literacy for research, GenAI use guidelines, Applied linguistics journals, Scholarly publishing},
abstract = {The unannounced appearance of GenAI in 2022 and the speed of its adoption by researchers have left many questions unanswered about its accepted ethical use, with no apparent consensus among applied linguists. In this context, it’s essential for researchers to develop their GenAI literacy for research to engage with GenAI effectively and responsibly. This study contributes to identifying key components of this literacy through examining accepted GenAI uses in research practices. Based on a systematically sampled collection of 170 high-impact journals in applied linguistics, we investigated the scope and nature of GenAI use guidelines provided by 76 journals intended to guide authors. A checklist including four items regarding general statements and 17 items regarding three categories of specific aspects that GenAI guidelines target (authorship, uses, and human responsibility) was identified. Our findings reveal that (1) less than half of the journals provided GenAI use guidelines to guide authors, (2) the number of specific aspects varied across journals, with most falling short of comprehensive coverage, and (3) disagreements were observed about whether AI can be cited and used for manuscript drafting, idea generating, image generating, data generation, data collection, and data analysis and interpretation. Additionally, journals varied in their guidance on how to disclose GenAI uses. We propose recommendations for journals in improving their AI guidelines. Importantly, we introduce and conceptualize the new construct GenAI literacy for research article writing (GenAI-LR) that is important for authors to develop. We provide actionable recommendations accordingly based on our findings.}
}
@article{MESSNER2025101622,
title = {Quantification of cultural practices and diversity: An empirical experiment with generative artificial intelligence},
journal = {Journal of World Business},
volume = {60},
number = {3},
pages = {101622},
year = {2025},
issn = {1090-9516},
doi = {https://doi.org/10.1016/j.jwb.2025.101622},
url = {https://www.sciencedirect.com/science/article/pii/S1090951625000112},
author = {Wolfgang Messner},
keywords = {Artificial intelligence, Cultural dimensions, Cultural diversity, Culture, Evolution, Generative artificial intelligence (genAI), GLOBE, Hofstede, Large language model (LLM)},
abstract = {Culture is often viewed as a value system that shapes cultural practices. Frameworks like Hofstede, GLOBE, and Schwartz identify and quantify various cultural dimensions; however, these rely on surveys that are criticized for limited country coverage, lack of psychometric robustness, small sample sizes, and cultural biases. This article presents an empirical experiment designed to quantify cultural practices and diversity across 216 countries and territories by prompting large language models using a zero-shot learning strategy. This approach enables subnational and segment-specific analyses, equipping researchers with powerful tools for deeper cultural insights.}
}
@article{METZGER2025103313,
title = {Generative artificial intelligence augmenting SME financial management},
journal = {Technovation},
volume = {147},
pages = {103313},
year = {2025},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2025.103313},
url = {https://www.sciencedirect.com/science/article/pii/S0166497225001452},
author = {Michael Metzger and Seán O'Reilly and Ciarán {Mac an Bhaird}},
keywords = {Financial management, SMEs, Artificial intelligence, Digital technologies, Predictive modelling, Going concern},
abstract = {This study investigates the potential for entrepreneurs to leverage advances in technological innovation, specifically generative Artificial Intelligence (AI), to build management capability to mitigate business and financial risks. Drawing on theories of Technology Affordances and Constraints and the Resource-Based View (RBV) of the firm, recognising that small and medium-sized enterprises (SMEs) are inherently resource-constrained. We examine how AI-generated financial diagnostics can empower SMEs by generating accessible, real-time analysis and insights, thus bolstering the management function and increasing chances of survival and growth. Using a dataset of 1,150 UK SMEs spanning eight years of financial statements, we test a large language model (LLM) prediction assessment and analyse the potential for SMEs to utilise the technology, notwithstanding enterprise-specific constraints. We conclude that AI may be a very effective tool for smaller enterprises to augment the financial management function, although its efficacy hinges on organisational readiness, competence in interpreting data, and the will to act on automated red-flag alerts. These findings offer practical guidance for SMEs seeking to enhance their financial management processes in today's digital era.}
}
@article{HUANG2025445,
title = {Ophthalmology Journals’ Guidelines on Generative Artificial Intelligence: A Comprehensive Analysis},
journal = {American Journal of Ophthalmology},
volume = {271},
pages = {445-454},
year = {2025},
issn = {0002-9394},
doi = {https://doi.org/10.1016/j.ajo.2024.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0002939424005889},
author = {Wenqiao Huang and Yating Liang and Xianghui Wei and Yi Du},
abstract = {Purpose
The integration of generative artificial intelligence (GAI) into scientific research and academic writing has generated considerable controversy. Currently, standards for using GAI in academic medicine remain undefined. This study aims to conduct a comprehensive analysis of the guidance provided for authors regarding the use of GAI in ophthalmology scientific journals.
Design
Cross-sectional bibliometric analysis.
Participants
A total of 140 ophthalmology journals listed in the Scimago Journal and Country Rankings, regardless of language or origin.
Methods
We systematically searched and screened the 140 ophthalmology journals’ websites on October 19 and 20, 2024, and conducted updates on November 19 and 20, 2024.
Main Outcome Measures
The content of GAI guidelines from the websites of the 140 ophthalmology journals.
Results
Of 140 journals reviewed, 96 (69%) provide explicit guidelines for authors regarding the use of GAI. Among these, nearly all journals agree on 3 key points: (1) 94 journals (98%) have established specific guidelines prohibiting GAI from being listed as an author; (2) 94 journals (98%) emphasize that human authors are responsible for the outputs generated by GAI tools; and (3) all 96 journals require authors to disclose any use of GAI. In addition, 20 journals (21%) specify that their guidelines pertain solely to the writing process with GAI. Furthermore, 92 journals (66%) have developed guidelines concerning GAI-generated images, with 63 journals (68%) permitting their use and 29 (32%) prohibiting them. Among those that prohibit GAI images, 27 journals (93%) allow their use under specific conditions.
Conclusion
Although there is considerable ethical consensus among ophthalmology journals regarding the use of GAI, notable variations exist in terms of permissible use and disclosure practices. Establishing standardized guidelines is essential to safeguard the originality and integrity of scientific research. Researchers must uphold high standards of academic ethics and integrity when using GAI.}
}
@article{SEXTON2024606,
title = {Assessments of Generative Artificial Intelligence as Clinical Decision Support Ought to be Incorporated Into Randomized Controlled Trials of Electronic Alerts for Acute Kidney Injury},
journal = {Mayo Clinic Proceedings: Digital Health},
volume = {2},
number = {4},
pages = {606-610},
year = {2024},
issn = {2949-7612},
doi = {https://doi.org/10.1016/j.mcpdig.2024.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2949761224001019},
author = {Donal J. Sexton and Conor Judge}
}
@article{TAIWO2025672,
title = {Generative artificial intelligence in construction: A Delphi approach, framework, and case study},
journal = {Alexandria Engineering Journal},
volume = {116},
pages = {672-698},
year = {2025},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.12.079},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824016776},
author = {Ridwan Taiwo and Idris Temitope Bello and Sulemana Fatoama Abdulai and Abdul-Mugis Yussif and Babatunde Abiodun Salami and Abdullahi Saka and Mohamed El Amine {Ben Seghier} and Tarek Zayed},
keywords = {Generative artificial intelligence, Generative pre-trained transformer, Large language model, Multimodal AI, Retrieval augmented generation, Construction industry, GenAI, RAG, LLM, GPT, ChatGPT},
abstract = {The construction industry plays a crucial role in the global economy, contributing approximately $10 trillion and employing over 220 million workers worldwide, but encounters numerous productivity challenges with only 1 % annual growth compared to 2.8 % for the global economy. These challenges span various processes, including design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (GenAI), capable of producing new and realistic data or content such as text, images, videos, or code from given inputs or existing knowledge, presents innovative solutions to these challenges. While there is an increasing interest in the applications of GenAI in construction, a detailed analysis of its practical uses, advantages, and areas ripe for development is still evolving. This study contributes to this emerging area by offering an insightful analysis of the current state of generative AI in construction. It has three objectives: (1) to identify and categorize the existing and emerging generative AI opportunities and challenges in the construction industry via a Delphi study; (2) to propose a framework enabling construction firms to build customized GenAI solutions; and (3) to illustrate this framework through a case study that employs GenAI model for querying contract documents. Through systematic review and expert consultation, the study identified 76 potential GenAI applications across construction phases and 18 key challenges distributed across domain-specific, technological, adoption, and ethical categories. The case study's findings show that retrieval augmented generation (RAG) improves the baseline large language model (LLM), GPT-4, by 5.2, 9.4, and 4.8 % in terms of quality, relevance, and reproducibility. The study recommends a structured approach to GenAI implementation, emphasizing the need for domain-specific customization, robust validation protocols, and careful consideration of ethical implications. This study equips academics and construction professionals with a comprehensive analysis and practical framework, facilitating the integration of GenAI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.}
}
@article{LI2025,
title = {Comparative Analysis of Generative Artificial Intelligence Systems in Solving Clinical Pharmacy Problems: Mixed Methods Study},
journal = {JMIR Medical Informatics},
volume = {13},
year = {2025},
issn = {2291-9694},
doi = {https://doi.org/10.2196/76128},
url = {https://www.sciencedirect.com/science/article/pii/S2291969425001401},
author = {Lulu Li and Pengqiang Du and Xiaojing Huang and Hongwei Zhao and Ming Ni and Meng Yan and Aifeng Wang},
keywords = {artificial intelligence, DeepSeek-R1, clinical pharmacy, comparative analysis, generative AI},
abstract = {Background
Generative artificial intelligence (AI) systems are increasingly deployed in clinical pharmacy; yet, systematic evaluation of their efficacy, limitations, and risks across diverse practice scenarios remains limited.
Objective
This study aims to quantitatively evaluate and compare the performance of 8 mainstream generative AI systems across 4 core clinical pharmacy scenarios—medication consultation, medication education, prescription review, and case analysis with pharmaceutical care—using a multidimensional framework.
Methods
Forty-eight clinically validated questions were selected via stratified sampling from real-world sources (eg, hospital consultations, clinical case banks, and national pharmacist training databases). Three researchers simultaneously tested 8 different generative AI systems (ERNIE Bot, Doubao, Kimi, Qwen, GPT-4o, Gemini-1.5-Pro, Claude-3.5-Sonnet, and DeepSeek-R1) using standardized prompts within a single day (February 20, 2025). A double-blind scoring design was used, with 6 experienced clinical pharmacists (≥5 years experience) evaluating the AI responses across 6 dimensions: accuracy, rigor, applicability, logical coherence, conciseness, and universality, scored 0‐10 per predefined criteria (eg, −3 for inaccuracy and −2 for incomplete rigor). Statistical analysis used one-way ANOVA with Tukey Honestly Significant Difference (HSD) post hoc testing and intraclass correlation coefficients (ICC) for interrater reliability (2-way random model). Qualitative thematic analysis identified recurrent errors and limitations.
Results
DeepSeek-R1 (DeepSeek) achieved the highest overall performance (mean composite score: medication consultation 9.4, SD 1.0; case analysis 9.3, SD 1.0), significantly outperforming others in complex tasks (P<.05). Critical limitations were observed across models, including high-risk decision errors—75% omitted critical contraindications (eg, ethambutol in optic neuritis) and a lack of localization—90% erroneously recommended macrolides for drug-resistant Mycoplasma pneumoniae (China’s high-resistance setting), while only DeepSeek-R1 aligned with updated American Academy of Pediatrics (AAP) guidelines for pediatric doxycycline. Complex reasoning deficits: only Claude-3.5-Sonnet detected a gender-diagnosis contradiction (prostatic hyperplasia in female); no model identified diazepam’s 7-day prescription limit. Interrater consistency was lowest for conciseness in case analysis (ICC=0.70), reflecting evaluator disagreement on complex outputs. ERNIE Bot (Baidu) consistently underperformed (case analysis: 6.8, SD 1.5; P<.001 vs DeepSeek-R1).
Conclusions
While generative AI shows promise as a pharmacist assistance tool, significant limitations—including high-risk errors (eg, contraindication omissions), inadequate localization, and complex reasoning gaps—preclude autonomous clinical decision-making. Performance stratification highlights DeepSeek-R1’s current advantage, but all systems require optimization in dynamic knowledge updating, complex scenario reasoning, and output interpretability. Future deployment must prioritize human oversight (human-AI co-review), ethical safeguards, and continuous evaluation frameworks.}
}
@article{PAN2025110175,
title = {Enhanced feedback analysis of vertical load reliability parameters for airplane landing gear using an improved generative adversarial network and explainable artificial intelligence techniques},
journal = {Engineering Applications of Artificial Intelligence},
volume = {145},
pages = {110175},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110175},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001757},
author = {Weihuang Pan and Yunwen Feng and Cheng Lu and Jiaqi Liu and Jingcui Liang},
keywords = {Landing gear vertical load, Operation reliability, Improved generative adversarial Network, Explainable artificial intelligence, Feedback analysis, Shapley Additive explanations, Data generation},
abstract = {Effective feedback analysis of critical equipment data is essential for improving performance and optimizing design parameters in aviation systems. This study presents a novel framework that integrates an improved generative adversarial network (GAN) with explainable artificial intelligence techniques (XAI) to evaluate the reliability of the vertical load for airplane landing gear. By utilizing limited data from the Quick Access Recorder (QAR), the improved GAN generates extensive synthetic data to expand the dataset and strengthen the analysis. Each parameter's importance and influence on vertical load reliability are then evaluated through the Shapley Additive Explanations (SHAP) method, a key approach in XAI. Validation using landing gear data from a typical civil airplane demonstrates the effectiveness of this method and confirms the viability of explainable artificial intelligence for parametric feedback analysis. The results highlight the impact of each parameter on vertical load reliability, providing valuable insights to support enhanced design and operational efficiency of landing gear.}
}
@article{TAIWO2025100316,
title = {Making waves: Generative artificial intelligence in water distribution networks: Opportunities and challenges},
journal = {Water Research X},
volume = {28},
pages = {100316},
year = {2025},
issn = {2589-9147},
doi = {https://doi.org/10.1016/j.wroa.2025.100316},
url = {https://www.sciencedirect.com/science/article/pii/S2589914725000155},
author = {Ridwan Taiwo and Abdul-Mugis Yussif and Tarek Zayed},
keywords = {Digital water systems, Generative artificial intelligence, Smart water distribution networks, Retrieval-augmented generation, ChatGPT, Reclaimed WDNs, RAG, Multimodal AI},
abstract = {Water distribution networks (WDNs) face increasing challenges from aging infrastructure, population growth, and climate change, necessitating innovative technological solutions. This study examines the integration of Generative Artificial Intelligence (GenAI) in WDNs, including both conventional and reclaimed water systems. Through a comprehensive analysis of current literature and emerging applications, the study identifies key opportunities in near-future applications focusing on enhancing information retrieval through advanced document processing, improving water quality management via real-time monitoring and visualization, implementing predictive maintenance strategies through pattern recognition, and optimizing real-time operational control through adaptive algorithms. Results also demonstrate that GenAI can transform WDN operations through advanced visualization, scenario generation, and adaptive optimization capabilities, particularly in far-future applications such as demand forecasting, emergency response, and network design optimization. The analysis reveals significant challenges, including data quality and availability issues, particularly in non-English speaking regions, scalability constraints in large-scale networks, the critical need for water professionals with hybrid expertise in both traditional engineering and AI systems, and complex regulatory requirements that vary significantly across the globe. The study also explores unique applications in reclaimed WDNs, particularly in quality control, treatment optimization, and stakeholder engagement. These findings provide water utilities, policymakers, and researchers with valuable insights for implementing GenAI technologies while balancing technological advancement with human expertise and social responsibility.}
}
@article{YOUNG2025200,
title = {A Hands-Free Approach With Voice to Text and Generative Artificial Intelligence: Streamlining Radiology Reporting},
journal = {Journal of the American College of Radiology},
volume = {22},
number = {2},
pages = {200-203},
year = {2025},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S154614402400838X},
author = {Austin Young and Katherine E. Wang and Michael X. Jin and Kian Avilla and Kevin Gilotra and Pamela Nguyen and Pablo R. Ros}
}
@article{DORTAGONZALEZ2024102187,
title = {Generative artificial intelligence usage by researchers at work: Effects of gender, career stage, type of workplace, and perceived barriers},
journal = {Telematics and Informatics},
volume = {94},
pages = {102187},
year = {2024},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2024.102187},
url = {https://www.sciencedirect.com/science/article/pii/S0736585324000911},
author = {Pablo Dorta-González and Alexis Jorge López-Puig and María Isabel Dorta-González and Sara M. González-Betancor},
keywords = {Artificial intelligence, Use of AI by researchers in the workplace, Challenges in implementing AI, Gender imbalance},
abstract = {The integration of generative artificial intelligence technology into research environments has become increasingly common in recent years, representing a significant shift in the way researchers approach their work. This paper seeks to explore the factors underlying the frequency of use of generative AI amongst researchers in their professional environments. As survey data may be influenced by a bias towards scientists interested in AI, potentially skewing the results towards the perspectives of these researchers, this study uses a regression model to isolate the impact of specific factors such as gender, career stage, type of workplace, and perceived barriers to using AI technology on the frequency of use of generative AI. It also controls for other relevant variables such as direct involvement in AI research or development, collaboration with AI companies, geographic location, and scientific discipline. Our results show that researchers who face barriers to AI adoption experience an 11 % increase in tool use, while those who cite insufficient training resources experience an 8 % decrease. Female researchers experience a 7 % decrease in AI tool usage compared to men, while advanced career researchers experience a significant 19 % decrease. Researchers associated with government advisory groups are 45 % more likely to use AI tools frequently than those in government roles. Researchers in for-profit companies show an increase of 19 %, while those in medical research institutions and hospitals show an increase of 16 % and 15 %, respectively. This paper contributes to a deeper understanding of the mechanisms driving the use of generative AI tools amongst researchers, with valuable implications for both academia and industry.}
}
@article{ZHANG2025100040,
title = {Generative artificial intelligence (AI) in built environment design and planning – A state-of-the-art review},
journal = {Progress in Engineering Science},
volume = {2},
number = {1},
pages = {100040},
year = {2025},
issn = {2950-4252},
doi = {https://doi.org/10.1016/j.pes.2024.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2950425224000409},
author = {Haolan Zhang and Ruichuan Zhang},
keywords = {Generative artificial intelligence (AI), Built environment design, Deep learning, Data-driven design, Benchmarking},
abstract = {Despite numerous studies on adopting, implementing, and developing generative design approaches within the architectural, engineering, and construction (AEC) sectors, there remains a limited understanding of the capabilities and constraints of generative artificial intelligence (AI) in specific applications for built environment design and planning. This review paper aims to bridge this gap by providing a systematic review guided by a framework encompassing three main related application areas in building development – site layout, interior, and exterior design, and three main categories of generative AI algorithms – rule-based AI and expert systems, optimization and metaheuristics, and machine learning algorithms, with a focus on state-of-the-art deep learning algorithms. We collected, reviewed, and analyzed 179 state-of-the-art studies in the past decade, consolidating siloed knowledge of user-centric design constraints and objectives, hybrid generative AI methods, data sources for development and testing, as well as benchmarking methods and metrics for assessing design performance, thereby providing a comprehensive understanding of the efficacy of generative AI technologies across diverse design contexts.}
}
@article{ZHOU2025103953,
title = {Government adoption of generative artificial intelligence and ambidextrous innovation},
journal = {International Review of Economics & Finance},
volume = {98},
pages = {103953},
year = {2025},
issn = {1059-0560},
doi = {https://doi.org/10.1016/j.iref.2025.103953},
url = {https://www.sciencedirect.com/science/article/pii/S1059056025001169},
author = {Zhikai Zhou and Dewen Liu and Zhongjie Chen and Martin Pancho},
keywords = {Generative artificial intelligence, TOE framework, Technology adoption, Organizational ambidextrous innovation},
abstract = {Every information technological revolution has brought about new possibilities for governmental organizational innovation, and the rapid development of Generative artificial intelligence (Gen-AI) is poised to profoundly impact government governance models and public service supply methods. Understanding the factors influencing government adoption of Gen-AI, and analyzing the impact of such adoption on governmental organizational innovation behavior, have emerged as urgent and cutting-edge topics. Based on the Technology-Organization-Environment (TOE) framework and the ambidextrous organization theory, this study systematically analyzes the three-layered driving factors that influence government organizations' adoption of Gen-AI, and examines the impact of Gen-AI on exploratory and exploitative innovation within government organizations. Furthermore, it delves into the influence mechanisms of technology adoption on different innovation behaviors from the meso-institutional and micro-implementation perspectives. At the theoretical level, this study constructs a conceptual framework for understanding the adoption of Gen-AI technology, extends the application scope of the TOE theory and enhances its explanatory power, while also providing new insights into the complexity of technology-enabled organizational innovation. At the practical level, it offers a more strategic perspective and profound implications for government organizations to maintain innovative vitality and achieve sustainable development amidst the wave of intelligent transformation.}
}
@article{DING2025100866,
title = {Tracking the carbon footprint of global generative artificial intelligence},
journal = {The Innovation},
volume = {6},
number = {5},
pages = {100866},
year = {2025},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2025.100866},
url = {https://www.sciencedirect.com/science/article/pii/S2666675825000694},
author = {Zhaohao Ding and Jianxiao Wang and Yiyang Song and Xiaokang Zheng and Guannan He and Xiupeng Chen and Tiance Zhang and Wei-Jen Lee and Jie Song}
}
@article{TRIPATHI2025,
title = {Toward Pediatric Patient–Friendly Education Material Using Generative Artificial Intelligence},
journal = {Journal of the American College of Radiology},
year = {2025},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2025.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1546144025004041},
author = {Satvik Tripathi and Dana Alkhulaifat and Hansel J. Otero and Tessa S. Cook}
}
@article{ROBINSON2025212,
title = {Generative Artificial Intelligence in Academic Surgery: Ethical Implications and Transformative Potential},
journal = {Journal of Surgical Research},
volume = {307},
pages = {212-220},
year = {2025},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2024.12.059},
url = {https://www.sciencedirect.com/science/article/pii/S0022480425000216},
author = {Jamie R. Robinson and Anne Stey and David F. Schneider and Anai N. Kothari and Brenessa Lindeman and Haytham M. Kaafarani and Krista L. Haines},
keywords = {AI, Artificial intelligence, ChatGPT, Generative AI, Large language models},
abstract = {Artificial intelligence (AI) is rapidly being used in medicine due to its advanced capabilities in image and video recognition, clinical decision support, surgical education, and administrative task automation. Large language models such as OpenAI’s Generative Pretrained Transformer (GPT)-4 and Google’s Bard have particularly revolutionized text generation, offering substantial benefits for the academic surgeon, including aiding in manuscript and grant writing. However, integrating AI into academic surgery necessitates addressing ethical concerns such as bias, transparency, and intellectual property. This paper provides guidelines and recommendations based on current literature around the opportunities and ethical challenges of AI in academic surgery. We discuss the underlying mechanisms of large language models, their potential biases, and the importance of responsible usage. Furthermore, we explore the ethical implications of AI in clinical documentation, highlighting improved efficiency and necessary privacy concerns. This review also addresses the critical issue of intellectual property dilemmas posed by AI-generated innovations in university settings. Finally, we propose guidelines for the responsible adoption of AI in academic and clinical environments, stressing the need for transparency, ethical training, and robust governance frameworks to ensure AI enhances, rather than undermines, academic integrity and patient care.}
}
@article{WANG2024102744,
title = {Green entrepreneurship success in the age of generative artificial intelligence: The interplay of technology adoption, knowledge management, and government support},
journal = {Technology in Society},
volume = {79},
pages = {102744},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102744},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002926},
author = {Shaofeng Wang and Hao Zhang},
keywords = {Generative artificial intelligence, Green entrepreneurship, Knowledge management, Green innovation, Government support, Resource orchestration theory},
abstract = {This study investigates the integral role of generative artificial intelligence (GAI) in enhancing green entrepreneurship success, focusing on the interconnected dynamics of GAI adoption, green knowledge management, innovation, and government support. Despite the growing interest in GAI, existing research lacks an understanding of how GAI fosters green entrepreneurship success, particularly in green knowledge management and innovation pathways. Utilizing a robust theoretical framework grounded in resource orchestration and knowledge management theories, we examine the influence of GAI on acquiring and applying green knowledge and its subsequent impact on fostering green innovation. The study examines how government funding moderates these correlations. Employing PLS-SEM and fsQCA, the research elucidates complex interrelationships and causal paths. The findings reveal that GAI significantly enhances green knowledge management capabilities, which drives green innovation and entrepreneurship success. Additionally, government support plays a crucial role in amplifying these effects. This study contributes to technological change and social transformation discourse, offering practical insights for decision-makers and stakeholders in green entrepreneurship and policy-making.}
}
@article{YANG2025,
title = {Reinforcement learning-based generative artificial intelligence for novel pesticide design},
journal = {Journal of Advanced Research},
year = {2025},
issn = {2090-1232},
doi = {https://doi.org/10.1016/j.jare.2025.02.030},
url = {https://www.sciencedirect.com/science/article/pii/S2090123225001286},
author = {Ruoqi Yang and Biao Li and Jin Dong and Zhuomei Cai and Hongyan Lin and Fan Wang and Guangfu Yang},
keywords = {Generative model, Reinforcement learning, Pesticide design, 4-hydroxyphenylpyruvate dioxygenase},
abstract = {Introduction
Pesticides play a pivotal role in ensuring food security, and the development of green pesticides is an inevitable trend in global agricultural progress. Although deep learning-based generative models have revolutionized de novo drug design in pharmaceutical research, their application in pesticide research and development remains unexplored.
Objectives
This study aims to pioneer the application of generative artificial intelligence to pesticide design by proposing a reinforcement learning-based framework for obtaining pesticide-like molecules with high binding affinity.
Methods
This framework comprises two key components: PestiGen-G, which systematically explores the pesticide-like chemical space using a character-based generative model coupled with the REINFORCE algorithm; and PestiGen-S, which combines a fragment-based generative model with the Monte Carlo Tree Search algorithm to generate molecules that stably bind to the specific target protein.
Results
Experimental results show that the molecules generated by PestiGen have superior pesticide-likeness and binding affinity compared to those generated by existing methods. In addition, we employ an active learning strategy to reduce the false-positive rate of the generated molecules. Finally, through collaboration with domain experts, we successfully designed a novel 4-hydroxyphenylpyruvate dioxygenase inhibitor (YH23768) with favorable enzyme inhibition and herbicidal potency.
Conclusion
This proof-of-concept study highlights the utility of PestiGen as a valuable tool for pesticide design. The web server based on the model is freely available at https://dpai.ccnu.edu.cn/PestiGen/.}
}
@article{LIU2025114513,
title = {A friend or a foe? The effect of generative artificial intelligence on creator contributions on original work sharing platforms},
journal = {Decision Support Systems},
volume = {197},
pages = {114513},
year = {2025},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2025.114513},
url = {https://www.sciencedirect.com/science/article/pii/S0167923625001149},
author = {Shan Liu and Wenxuan Hu and Baojun Gao},
keywords = {Generative artificial intelligence, Copyright infringement, Original work sharing platform, Crowding out, Protective motivation theory},
abstract = {While generative artificial intelligence (GAI) is increasingly used to create content, it is often criticized for collecting and training private data and induces potential copy infringement issue. This dilemma leaves a question of whether GAI increases or decreases creators' work sharing. Drawn on protection motivation theory, this study examines how the launch of a GAI system affects creators' contributions on an original work sharing platform. We discover that GAI poses a threat to drawing-category creators, leading to a significant crowding-out effect on their contributions. Specifically, compared with that of non-drawing-category creators, the work sharing of drawing-category creators decreases by 19.64 % and 14.29 % within a short period after the launch and removal of the GAI system, respectively. We discover that creators' protective behavior is driven by GAI-related copyright infringement. Compared with creators without copyright protection, those with copyright protection are more inclined to cease contributions or even leave the platform. We further find that among copyright-protected creators, top creators, evidenced by their acquisition of a large number of supporters or platform honor titles, exhibit more pronounced responses to protect their works due to their higher coping efficacy. Notably, this threat reduces creators' sharing behavior or even lead to their exit from the platform. Nevertheless, such reduction is likely to gradually recover once the threat subsides. Overall, our findings have important implications for whether and how platform managers adopt GAI systems, especially in an original work sharing context.}
}
@article{RODLER20241496,
title = {Generative artificial intelligence in surgery},
journal = {Surgery},
volume = {175},
number = {6},
pages = {1496-1502},
year = {2024},
issn = {0039-6060},
doi = {https://doi.org/10.1016/j.surg.2024.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0039606024001193},
author = {Severin Rodler and Conner Ganjavi and Pieter {De Backer} and Vasileios Magoulianitis and Lorenzo Storino Ramacciotti and Andre Luis {De Castro Abreu} and Inderbir S. Gill and Giovanni E. Cacciamani},
abstract = {Generative artificial intelligence is able to collect, extract, digest, and generate information in an understandable way for humans. As the first surgical applications of generative artificial intelligence are applied, this perspective paper aims to provide a comprehensive overview of current applications and future perspectives for the application of generative artificial intelligence in surgery, from preoperative planning to training. Generative artificial intelligence can be used before surgery for planning and decision support by extracting patient information and providing patients with information and simulation regarding the procedure. Intraoperatively, generative artificial intelligence can document data that is normally not captured as intraoperative adverse events or provide information to help decision-making. Postoperatively, GAIs can help with patient discharge and follow-up. The ability to provide real-time feedback and store it for later review is an important capability of GAIs. GAI applications are emerging as highly specialized, task-specific tools for tasks such as data extraction, synthesis, presentation, and communication within the realm of surgery. GAIs have the potential to play a pivotal role in facilitating interaction between surgeons and artificial intelligence.}
}
@article{HERZBERG2025102363,
title = {Assessing the standard-essentiality of 5G technology patents by means of generative artificial intelligence},
journal = {World Patent Information},
volume = {81},
pages = {102363},
year = {2025},
issn = {0172-2190},
doi = {https://doi.org/10.1016/j.wpi.2025.102363},
url = {https://www.sciencedirect.com/science/article/pii/S0172219025000304},
author = {Andre Herzberg},
abstract = {In telecommunication technology, identifying standard-essential patents (SEPs) plays a crucial role in the management of intellectual property. This technology is regulated by technical standards that are largely based on the content of SEPs. These patents are declared standard-essential by their owners because they contain elements of a technical standard. The declaration process leaves room for over- and under-declaration, which entails risks for organizations. This paper focuses on the question of how generative artificial intelligence can be used to assess the standard-essentiality of 5G technology patents. For this purpose, the standard-essentiality is assessed using different prompts with four Large Language Models (LLMs) in two variants. In the first variant, the LLM results are generated by a rather simple prompt and compared with an approach based on unsupervised and supervised machine learning. The result shows that large LLMs are capable of assessing the standard-essentiality. In the second variant, the best-performing LLM is selected and the prompt is expanded to include selected parts of a technical standard. While the assessment results remain largely the same, the LLM is now able to explain in which detail a patent is part of a standard. This has several implications for patent evaluation, licensing and litigation strategies.}
}
@article{FENG2024100090,
title = {Latest developments of generative artificial intelligence and applications in ophthalmology},
journal = {Asia-Pacific Journal of Ophthalmology},
volume = {13},
number = {4},
pages = {100090},
year = {2024},
issn = {2162-0989},
doi = {https://doi.org/10.1016/j.apjo.2024.100090},
url = {https://www.sciencedirect.com/science/article/pii/S2162098924000914},
author = {Xiaoru Feng and Kezheng Xu and Ming-Jie Luo and Haichao Chen and Yangfan Yang and Qi He and Chenxin Song and Ruiyao Li and You Wu and Haibo Wang and Yih Chung Tham and Daniel Shu Wei Ting and Haotian Lin and Tien Yin Wong and Dennis Shun-chiu Lam},
keywords = {Generative artificial intelligence, Ophthalmology, Risk management, Clinical workflow, AI in medical research},
abstract = {The emergence of generative artificial intelligence (AI) has revolutionized various fields. In ophthalmology, generative AI has the potential to enhance efficiency, accuracy, personalization and innovation in clinical practice and medical research, through processing data, streamlining medical documentation, facilitating patient-doctor communication, aiding in clinical decision-making, and simulating clinical trials. This review focuses on the development and integration of generative AI models into clinical workflows and scientific research of ophthalmology. It outlines the need for development of a standard framework for comprehensive assessments, robust evidence, and exploration of the potential of multimodal capabilities and intelligent agents. Additionally, the review addresses the risks in AI model development and application in clinical service and research of ophthalmology, including data privacy, data bias, adaptation friction, over interdependence, and job replacement, based on which we summarized a risk management framework to mitigate these concerns. This review highlights the transformative potential of generative AI in enhancing patient care, improving operational efficiency in the clinical service and research in ophthalmology. It also advocates for a balanced approach to its adoption.}
}
@article{KANAKALA2024103175,
title = {Generative artificial intelligence for small molecule drug design},
journal = {Current Opinion in Biotechnology},
volume = {89},
pages = {103175},
year = {2024},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2024.103175},
url = {https://www.sciencedirect.com/science/article/pii/S0958166924001113},
author = {Ganesh Chandan Kanakala and Sriram Devata and Prathit Chatterjee and Udaykumar Deva Priyakumar},
abstract = {In recent years, the rapid advancement of generative artificial intelligence (GenAI) has revolutionized the landscape of drug design, offering innovative solutions to potentially expedite the discovery of novel therapeutics. GenAI encompasses algorithms and models that autonomously create new data, including text, images, and molecules, often mirroring characteristics of existing datasets. This comprehensive review delves into the realm of GenAI for drug design, emphasizing recent advancements and methodologies that have propelled the field forward. Specifically, we focus on three prominent paradigms: transformers, diffusion models, and reinforcement learning algorithms, which have been exceptionally impactful in the last few years. By synthesizing insights from a myriad of studies and developments, we elucidate the potential of these approaches in accelerating the drug discovery process. Through a detailed analysis, we explore the current state and future directions of GenAI in the context of drug design, highlighting its transformative impact on pharmaceutical research and development.}
}
@article{ECKHARDT2025100987,
title = {Livestock behaviour forecasting via generative artificial intelligence},
journal = {Smart Agricultural Technology},
volume = {11},
pages = {100987},
year = {2025},
issn = {2772-3755},
doi = {https://doi.org/10.1016/j.atech.2025.100987},
url = {https://www.sciencedirect.com/science/article/pii/S2772375525002205},
author = {Regina Eckhardt and Reza Arablouei and Aaron Ingham and Kieren McCosker and Heinz Bernhardt},
keywords = {Accelerometer data, Cattle behaviour, Data imputation, Generative AI, Precision agriculture},
abstract = {Recent advancements in sensor technology and generative artificial intelligence (AI) are transforming precision livestock farming by enhancing behaviour monitoring and predictive analytics. This study examines the effectiveness of Transformer-type generative AI models in predicting cattle behaviour profiles and imputing missing data from collar accelerometer readings collected during two trials in Queensland, Australia, in 2022 and 2023, alongside climatic data. Each trial involved 60 cattle equipped with collars that classified six core behaviours: grazing, ruminating, walking, resting, drinking, and other over five-second time windows. Hourly behaviour profiles were constructed for each animal and experiment day by aggregating the behaviour predictions over every calendar hour, representing the time spent on each behaviour within each hour. Subsequently, four Transformer-type models (i.e., standard Transformer, Informer, Reformer, and Autoformer) were trained on the hourly behaviour profile data to predict behaviour profiles of the next 24 hours for each animal. Among the considered models, Autoformer showed the highest predictive accuracy when including climate data, achieving a mean absolute error (MAE) of <5.5 min, while the next best model had an MAE of approximately 6 min. For imputing missing data, the standard Transformer outperformed traditional imputation methods, with an MAE of <30 min over 24 hours, compared to 40 to 70 min for traditional methods (mean, median, and linear interpolation). These results highlight the potential of generative AI, particularly Autoformer and Transformer, to enhance predictive accuracy and data imputation in livestock management, thereby supporting regulatory guidance for data-driven decision-making and improved farming practices.}
}
@article{TOROUS2025683,
title = {Assessing generative artificial intelligence for mental health},
journal = {The Lancet},
volume = {406},
number = {10504},
pages = {683},
year = {2025},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(25)01237-1},
url = {https://www.sciencedirect.com/science/article/pii/S0140673625012371},
author = {John Torous and Eric J Topol}
}
@article{CHO2025101418,
title = {Exploring international students' perceptions of adopting generative artificial intelligence (GenAI) technologies in learning},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101418},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101418},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125001457},
author = {Changwhan Cho and Duke Ofosu-Anim},
abstract = {This study explores international students' perceptions of GenAI technologies in higher education, focusing on how gender and age influence their willingness to adopt. A survey of 122 international graduate students from a private university in South Korea showed that the students are generally familiar with GenAI and its uses in learning. However, its usage varied in frequency. The study also finds that male students are more likely to use GenAI than female students. Additionally, the study revealed age-related differences in the willingness of international students to adopt GenAI, with younger students showing more interest and willingness than older students. However, despite the differences in interest levels in adopting GenAI among genders and ages, the study revealed that overall, there is a general willingness among international students to learn and apply GenAI technologies to their studies. The South Korean education system can be reformed to accommodate the emerging and growing relevance of GenAI in education by developing ethical capacities that will enhance learning while addressing students’ concerns.}
}
@article{WALLER2025102227,
title = {Reliable answers to patients’ questions: A fundamental need in any patient education tool, especially generative artificial intelligence},
journal = {Journal of Nuclear Cardiology},
volume = {47},
pages = {102227},
year = {2025},
issn = {1071-3581},
doi = {https://doi.org/10.1016/j.nuclcard.2025.102227},
url = {https://www.sciencedirect.com/science/article/pii/S1071358125001011},
author = {Alfonso H. Waller and Baoqiong Liu},
keywords = {Artificial intelligence, Generative artificial intelligence, ChatGPT, Nuclear stress, Fluorodeoxyglucose, Positron emission tomography}
}
@article{HAMADA2025174,
title = {In silico design of smaller size enzymatic protein by generative artificial intelligence (ProtGPT2)},
journal = {Journal of Bioscience and Bioengineering},
volume = {140},
number = {3},
pages = {174-179},
year = {2025},
issn = {1389-1723},
doi = {https://doi.org/10.1016/j.jbiosc.2025.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389172325001598},
author = {Hiroyuki Hamada and Tamon Matsuzawa and Taizo Hanai},
keywords = {Generative artificial intelligence, Protein language model, ProtGPT2, Amino acid sequence design, Smaller size protein, Malate dehydrogenase},
abstract = {The construction of small proteins by removing amino acid subsequences that are not involved in function, activity, or structure is crucial for bioprocessing and drug development. Traditional design methods often focus on reconstructing functional motifs, but they face challenges in stabilizing structure and reproducing function. In this study, we aimed to develop a design method for small proteins using ProtGPT2, a model that generates protein sequences based on function and structure. First, amino acid sequence data of malate dehydrogenase (MDH) was collected, and ProtGPT2 was fine-tuned (ProtGPT2 for MDH). The chain length and perplexity (ppl) of the generated sequences were evaluated, producing shorter sequences than the natural ones. The validity of the generated sequences was assessed using both population and individual analyses. Population analysis, including multiple sequence alignment (MSA) and t-distributed stochastic neighbor embedding (tSNE), revealed that ProtGPT2 for MDH identified functional motifs of MDH and incorporated them into the generated sequences. Additionally, tSNE showed that the generated sequences were highly similar to natural MDH sequences. In individual analysis, 10 randomly selected sequences were evaluated using BLAST, AlphaFold2, and InterPro. BLAST indicated that 9 sequences were novel MDH variants. AlphaFold2 confirmed that their 3D structures were highly similar to known MDH structures. InterPro identified domains and active sites in 2 sequences, suggesting that they were novel, small MDH variants. In conclusion, ProtGPT2 for MDH has the potential to design amino acid sequence candidates for small MDHs. The validity and utility of the model will be established through future experimental efforts.}
}
@article{SALARI2025100652,
title = {Impacts of generative artificial intelligence on the future of labor market: A systematic review},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100652},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100652},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000673},
author = {Nader Salari and Mahan Beiromvand and Amin Hosseinian-Far and Javad Habibi and Fateme Babajani and Masoud Mohammadi},
keywords = {Job market, AI, ChatGPT, Labor market, GenAI},
abstract = {Background
Generative AI (GenAI) has the ability to autonomously collect and process data to generate contents, inform decisions, solve problems, and perform tasks that typically require human reasoning. This Systematic Review is conducted to examine the impacts of GenAI on the future of employment, focusing on concerns about rising unemployment, and the positive and negative perspectives outlined within exiting studies. The findings from this review can help identify research gaps, guide organizational planning, and improve AI governance frameworks and policies.
Methods
To identify relevant studies, the PubMed, Scopus, Web of Science, Embase, ScienceDirect and Google Scholar databases and repositories were systematically searched using the keywords: ‘Future of work’, ‘Job market’, ‘Generative AI’, ‘Generative AI’, and ‘ChatGPT’. Additionally, the reference lists of the identified related articles were reviewed for grey literature.
Results
Following the PRISMA guidelines, a total of 14 articles were selected for analysis. Selected studies have examined the positive and negative viewpoints on GenAI, together with pertinent challenges and opportunities. Accordingly, GenAI, when compliant with security and ethical issues, has the potential to increase efficiency whilst reducing costs and time.
Conclusion
Considering the rapid growth and adoption of AI technologies, examining the impacts of GenAI on the future of labor market is crucial. GenAI is likely to create new roles in some sectors yet reduce opportunities in others. A nuanced assessment of the impacts, and ongoing monitoring are vital for effective preparation and adaptation to the evolving work landscape in the presence of advanced AI technologies.}
}
@article{EHMKE2025222,
title = {Self-perceived knowledge, skills, and attitude of nursing faculty on generative artificial intelligence in nursing education: A descriptive, cross-sectional study},
journal = {Teaching and Learning in Nursing},
volume = {20},
number = {3},
pages = {222-227},
year = {2025},
issn = {1557-3087},
doi = {https://doi.org/10.1016/j.teln.2025.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S1557308725000447},
author = {Sabrina D. Ehmke and Jenny Bridges and Sarah E. Patel},
keywords = {Artificial intelligence, Nursing education, Nursing faculty},
abstract = {Background
AI is transforming health and education, offering innovative solutions to workflow and curriculum challenges. However, faculty members lack familiarity with AI, limiting their ability to prepare students for AI-driven healthcare.
Aim
Our study investigated nursing faculty's knowledge, skills, and attitudes toward integrating Artificial Intelligence (AI) into education, examining differences by degree type and the influence of policies or syllabi on AI integration.
Methods
A descriptive, cross-sectional study assessed faculty's self-perceptions of knowledge, skills, and attitudes regarding AI in education. Data were gathered via a survey of nursing faculty from diverse institutions.
Results
Findings revealed gaps in AI knowledge and skills linked to educational level and institutional policy development. Doctorly prepared-faculty reported higher perceived knowledge and skills, while BS-prepared faculty had higher attitudes toward AI. Faculty involved in AI policy or syllabus development perceived greater knowledge, skills, and attitudes.
Conclusion
Faculty education and policy support are critical for integrating AI into education. Institutions should invest in faculty development and ethical AI adoption, using case studies, simulation, and decision-support tools to enhance curriculum and healthcare outcomes.}
}
@article{ZHAO2025108654,
title = {“Positive” or “Threatened”? The impact of the features in generative artificial intelligence on continued behavior},
journal = {Computers in Human Behavior},
volume = {168},
pages = {108654},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108654},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225001013},
author = {Li Zhao and Yun Xu and Sheng-kai Zhou},
keywords = {Artificial intelligence generated content (AIGC), Positive awe, Threatened awe, Continued usage intention},
abstract = {Artificial intelligence technologies have empowered marketers with advanced tools and insights, fostering unparalleled efficiency and personalization decision-making. To provide marketers with targeted and actionable guidance, this study investigated the behavioral mechanisms underlying the adoption of artificial intelligence-generated content (AIGC) technology. Specifically, it examined the influence of AIGC features (accuracy, competence, anthropomorphism, and interactivity) and the distinct psychological mechanisms of awe on users' behavioral intentions. A mixed-methods approach was employed, combining quantitative data (N = 860) with qualitative research (user reviews). The analysis revealed that the awe experience significantly influences AIGC users' preferences to continue using the technology. Positive awe had a significant positive effect, while threatened awe had a comparatively weaker negative effect. The four features (accuracy, competence, anthropomorphism, and interactivity) of AIGC contribute significantly to its users' continued usage intention. Notably, positive awe induced by competence, anthropomorphism, and interactivity significantly outweighed threatened awe, with the exception of accuracy. The findings reveal that the unique features of AIGC not only evoke users’ perceived awe but also strengthen their intentions to continue using the technology.}
}
@article{BARROSO2025501667,
title = {Application of generative artificial intelligence chatbots in the field of anesthesia},
journal = {Revista Española de Anestesiología y Reanimación (English Edition)},
volume = {72},
number = {6},
pages = {501667},
year = {2025},
issn = {2341-1929},
doi = {https://doi.org/10.1016/j.redare.2025.501667},
url = {https://www.sciencedirect.com/science/article/pii/S2341192925001179},
author = {A. Barroso and R. Casans}
}
@article{TEO2024100091,
title = {Cybersecurity in the generative artificial intelligence era},
journal = {Asia-Pacific Journal of Ophthalmology},
volume = {13},
number = {4},
pages = {100091},
year = {2024},
issn = {2162-0989},
doi = {https://doi.org/10.1016/j.apjo.2024.100091},
url = {https://www.sciencedirect.com/science/article/pii/S2162098924000926},
author = {Zhen Ling Teo and Chrystie Wan Ning Quek and Joy Le Yi Wong and Daniel Shu Wei Ting},
keywords = {Generative Artificial Intelligence, ChatGPT, Cybersecurity, Privacy risks, Large language model},
abstract = {Generative Artificial Intelligence (GenAI) are algorithms capable of generating original content. The ability of GenAI to learn and generate novel outputs alike human cognition has taken the world by storm and ushered in a new era. In this review, we explore the role of GenAI in healthcare, including clinical, operational, and research applications, and delve into the cybersecurity risks of this technology. We discuss risks such as data privacy risks, data poisoning attacks, the propagation of bias, and hallucinations. In this review, we recommend risk mitigation strategies to enhance cybersecurity in GenAI technologies and further explore the use of GenAI as a tool in itself to enhance cybersecurity across the various AI algorithms. GenAI is emerging as a pivotal catalyst across various industries including the healthcare domain. Comprehending the intricacies of this technology and its potential risks will be imperative for us to fully capitalise on the benefits that GenAI can bring.}
}
@article{PALLOTTINO2025109919,
title = {Applications and perspectives of Generative Artificial Intelligence in agriculture},
journal = {Computers and Electronics in Agriculture},
volume = {230},
pages = {109919},
year = {2025},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2025.109919},
url = {https://www.sciencedirect.com/science/article/pii/S0168169925000250},
author = {Federico Pallottino and Simona Violino and Simone Figorilli and Catello Pane and Jacopo Aguzzi and Giacomo Colle and Eugenio {Nerio Nemmi} and Alessandro Montaghi and Damianos Chatzievangelou and Francesca Antonucci and Lavinia Moscovini and Alessandro Mei and Corrado Costa and Luciano Ortenzi},
keywords = {GAI, GAN, NLP, LLMs, ChatGPT, Microsoft Copilot},
abstract = {Artificial Intelligence (AI) applications related to agriculture have recently gained in use and attention. They are indeed valuable tools for interpreting data, improving production chains, and optimizing the use of natural resources. Among AI models, the most recent and promising area is represented by Generative Artificial Intelligence (GAI). After an initial description of its general model architectures, this work aims to review its practical uses and potentials in the following individual sectors: agriculture, precision farming, and animal farming, as well as interdisciplinary applications. The literature search was carried out using the SCOPUS, Google Scholar, and Web of Science databases. GAI holds immense potential for revolutionizing agriculture, offering solutions ranging from precision farming to pest management and supply chain optimization. Though some applications can extend beyond efficiency gains, and hallucinations occurrence i.e. false output information presented as fact, remains an open issue, GAI can be decisive for tasks like improving training datasets, refining models, and facilitating time series analysis. This review extensively describes the vital importance of these tasks for agriculture, precision and animal farming, caused by the rise of new technologies. As a result, by embracing and responsibly implementing GAI applications, it is possible to create a more sustainable and resilient future for agriculture and precision farming. GAI have the capacity to extract specific information from big data systems, offering huge potential to meet a growing global population demand and consequent environmental challenges for the future.}
}
@article{CHENG2025104194,
title = {From emotion to reflection: leveraging EmotionPrompt strategy to empower self-determination in decision-making with generative artificial intelligence},
journal = {Information & Management},
volume = {62},
number = {7},
pages = {104194},
year = {2025},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2025.104194},
url = {https://www.sciencedirect.com/science/article/pii/S0378720625000977},
author = {Xusen Cheng and Lu Gao and Xin (Robert) Luo},
keywords = {Human–AI interaction, EmotionPrompt, Regulatory focus theory, Cognitive reappraisal, Psychological empowerment},
abstract = {Communication and reflection abilities are critical in managing strategic cooperation between humans and Generative Artificial Intelligence (GAI), especially when facing conflict in decision-making. This study introduces two variations of EmotionPrompt strategies, drawing on regulatory focus theory, to explore both individuals' perceptions of GAI ability and their empowerment in self-competence when handling disagreements. An experiment between humans and GAI chatbots in determining product promotion strategy showed that emotional prompts impact individuals' reappraisals of both chatbots and their own performance profoundly, cultivating self-determination in the final decision. Importantly, EmotionPrompt with promotion orientation can increase the perceived flexibility of chatbot decision-makers, facilitating individual self-enhancement and trust in GAI competence. In contrast, the prevention-oriented EmotionPrompt appears to constrain individuals' judgments and decision-making processes, as evidenced by the increased occurrence of inhibit words and anxiety emotions in their reflections. These findings provide novel perspectives on implementing specific regulatory-oriented EmotionPrompt strategies in GAI to address opinion conflicts in decision-making with humans.}
}
@article{BOSCO2025,
title = {Designing a Multimodal and Culturally Relevant Alzheimer Disease and Related Dementia Generative Artificial Intelligence Tool for Black American Informal Caregivers: Cognitive Walk-Through Usability Study},
journal = {JMIR Aging},
volume = {8},
year = {2025},
issn = {2561-7605},
doi = {https://doi.org/10.2196/60566},
url = {https://www.sciencedirect.com/science/article/pii/S2561760525000027},
author = {Cristina Bosco and Ege Otenen and John {Osorio Torres} and Vivian Nguyen and Darshil Chheda and Xinran Peng and Nenette M Jessup and Anna K Himes and Bianca Cureton and Yvonne Lu and Carl V Hill and Hugh C Hendrie and Priscilla A Barnes and Patrick C Shih},
keywords = {multimodality, artificial intelligence, AI, generative AI, usability, black, African American, cultural, Alzheimer's, dementia, caregivers, mobile app, interaction, cognition, user opinion, geriatrics, smartphone, mHealth, digital health, aging},
abstract = {Background
Many members of Black American communities, faced with the high prevalence of Alzheimer disease and related dementias (ADRD) within their demographic, find themselves taking on the role of informal caregivers. Despite being the primary individuals responsible for the care of individuals with ADRD, these caregivers often lack sufficient knowledge about ADRD-related health literacy and feel ill-prepared for their caregiving responsibilities. Generative AI has become a new promising technological innovation in the health care domain, particularly for improving health literacy; however, some generative AI developments might lead to increased bias and potential harm toward Black American communities. Therefore, rigorous development of generative AI tools to support the Black American community is needed.
Objective
The goal of this study is to test Lola, a multimodal mobile app, which, by relying on generative AI, facilitates access to ADRD-related health information by enabling speech and text as inputs and providing auditory, textual, and visual outputs.
Methods
To test our mobile app, we used the cognitive walk-through methodology, and we recruited 15 informal ADRD caregivers who were older than 50 years and part of the Black American community living within the region. We asked them to perform 3 tasks on the mobile app (ie, searching for an article on brain health, searching for local events, and finally, searching for opportunities to participate in scientific research in their area), then we recorded their opinions and impressions. The main aspects to be evaluated were the mobile app’s usability, accessibility, cultural relevance, and adoption.
Results
Our findings highlight the users’ need for a system that enables interaction with different modalities, the need for a system that can provide personalized and culturally and contextually relevant information, and the role of community and physical spaces in increasing the use of Lola.
Conclusions
Our study shows that, when designing for Black American older adults, a multimodal interaction with the generative AI system can allow individuals to choose their own interaction way and style based upon their interaction preferences and external constraints. This flexibility of interaction modes can guarantee an inclusive and engaging generative AI experience.}
}
@article{CHENG2025100374,
title = {Asking generative artificial intelligence the right questions improves writing performance},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100374},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100374},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000141},
author = {Yixin Cheng and Yizhou Fan and Xinyu Li and Guanliang Chen and Dragan Gašević and Zachari Swiecki},
keywords = {Generative AI, ChatGPT, Question asking, Help seeking, Epistemic network analysis, Mediation analysis},
abstract = {Generative Artificial Intelligence (GenAI) tools are widely used by learners and this trend is poised to continue. However, little is known about whether and how GenAI use impacts learning and performance. This study aimed to investigate the effect of GenAI on performance by examining a key affordance of GenAI—seeking help via question asking. We compared the questions that learners asked GenAI versus a human tutor online during a writing task. Using quantitative ethnographic methods, we found that: (a) participants in the GenAI condition asked significantly more questions compared to those in the Tutor condition; (b) GenAI participants tended to ask one-off questions, while Tutor participants tended to have longer conversational exchanges; (c) GenAI participants tended to question pragmatically, asking direct questions about conceptual and procedural knowledge, while Tutor participants tended to make indirect request for feedback; (d) question asking, as measured by epistemic network analysis, mediated the relationship between experimental condition and performance—the more pragmatic the questions, and thus the more like questions in the GenAI condition, the better the performance; and (e) questions in the GenAI condition were driven by social coordination and knowledge deficits, while questions in the Tutor condition were driven by social coordination and establishing common ground. These findings suggest learners may be less hesitant to admit knowledge deficits and more willing to repair them when interacting with GenAI compared to human tutors. Thus, GenAI can be a useful educational tool when improved performance is the goal and human tutoring may benefit from creating a space where learners are more comfortable revealing a lack of knowledge.}
}
@article{DERAKHSHAN2025102114,
title = {EFL students’ perceptions about the role of generative artificial intelligence (GAI)-mediated instruction in their emotional engagement and goal orientation: A motivational climate theory (MCT) perspective in focus},
journal = {Learning and Motivation},
volume = {90},
pages = {102114},
year = {2025},
issn = {0023-9690},
doi = {https://doi.org/10.1016/j.lmot.2025.102114},
url = {https://www.sciencedirect.com/science/article/pii/S0023969025000219},
author = {Ali Derakhshan},
keywords = {Achievement goal theory, Emotional engagement, Generative artificial intelligence (GAI), Goal orientation, L2 education, Motivational climate theory (MCT), Self-determination theory (SDT)},
abstract = {Research on the contributions of generative artificial intelligence (GAI) technologies to second language (L2) education has soared in the past couple of years. However, there is limited evidence pertaining to the impact of AI-mediated instruction on postgraduate students’ psycho-affective factors and the overall learning climate in English as a foreign language (EFL) context. To address this gap, the present study drew on motivational climate theory (MCT) to explore postgraduate EFL students’ perceptions of the role of GAI technologies in their emotional engagement and goal orientation. To do so, an interview was conducted with 30 postgraduate students using maximum variation sampling. The results of the inductive thematic analysis revealed that AI-mediated instruction had affected both the emotional engagement and goal orientation of the students. In particular, it was found that GAI tools fostered emotional engagement by ‘enlightening teacher-student classroom relationships’, ‘making the overall classroom culture/climate engaging, motivating, and updated’, ‘improving teachers’ action, instruction, and feedback quality’, ‘providing a personalized, interactive, and autonomy supporting education’, and ‘taping into learner-specific idiosyncrasies and individual differences’. Furthermore, GAI tools affected the students’ goal orientation by ‘facilitating the mastery of course content’, ‘setting personalized and achievable goals’, ‘fostering students’ performance comparison in the classroom’, and ‘providing a reflective and adaptive learning environment’. The findings are discussed and implications are provided for EFL teachers, students, teacher educators, and policymakers concerning the interplay of GAI, emotions, goal orientation, and motivational climate.}
}
@article{JIN2025100467,
title = {Mechanisms of enhancing learning with unequal preparation: An experimental study on generative artificial intelligence use and proficiency in programming learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {9},
pages = {100467},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100467},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25001079},
author = {Yuan Jin and Wei He and Jun Shen and Jingyun Hu},
keywords = {Generative artificial intelligence (GenAI), Learning preparation, GenAI proficiency, Learning outcomes, Experimental study},
abstract = {Prior learning preparation, encompassing learners' prior knowledge, resources, and readiness for learning new material, plays a critical role in a new learning process. Unequal learning preparation commonly exists among learners with varying socioeconomic and academic backgrounds, and can further exacerbate the educational divide. By reducing learning cost and providing less prepared learners with personalized delivery of information and knowledge, Generative Artificial Intelligence (GenAI) may help mitigate disparities in learning preparation. In this study, we investigate the roles of learners' use of GenAI in their learning processes, considering varying levels of learners’ prior learning preparation and proficiency in GenAI use. Specifically, we examine the effects of the use of GenAI tools on learning outcomes through its impacts on perceived informational benefit, learning cost, and knowledge fit in the focal learning process, moderated by learning preparation and GenAI proficiency of learners. Based on an experiment in a programming learning context, we find that although GenAI use significantly reduces learning cost, especially for less prepared learners, the cost reduction effect has not been translated into improved learning outcomes. We find no significant moderating effects of learning preparation on how GenAI use affects informational benefit and knowledge fit, further showing that GenAI tools have not yet been effectively utilized to help less prepared learners or reduce the educational divide. As indicated by the significant moderating roles of GenAI proficiency, to fully leverage the power of GenAI, improving GenAI proficiency can be crucial to ensure learning effectiveness for learners with different levels of learning preparation.}
}
@article{MABWE2025820,
title = {Generative artificial intelligence chatbots in investment decision-making: a phantom menace or a new hope?},
journal = {Foresight},
volume = {27},
number = {4},
pages = {820-863},
year = {2025},
issn = {1463-6689},
doi = {https://doi.org/10.1108/FS-06-2024-0122},
url = {https://www.sciencedirect.com/science/article/pii/S1463668925000082},
author = {Kumbirai Mabwe and Nasir Aminu and Stanislav Hristov Ivanov and Diyan Dimov},
keywords = {Generative AI, ChatGPT, Bard, Gemini, Bing, Large language models, Chatbots, Investment recommendations},
abstract = {Purpose
This study aims to investigate the relevance, accuracy, specificity and justification of investment recommendations of generative artificial intelligence (GenAI) chatbots for different investment capitals and countries (UK and Bulgaria).
Design/methodology/approach
A two-stage mixed methods approach was used. Prompts were queried into OpenAI’s ChatGPT, Microsoft Bing and Google Bard (now Gemini). Finance and investment practitioners and finance and investment lecturers assessed the chatbots’ recommendations through an online questionnaire using a five-point Likert scale. The Chi-squared test, Wilcoxon-signed ranks test, Mann–Whitney U test and Friedman test were used for data analysis to compare GenAIs’ recommendations for the UK and Bulgaria across different amounts of investment capital and to assess the consistency of the chatbots.
Findings
GenAI chatbots’ responses were found to perform medium-to-high in terms of relevance, accuracy, specificity and justification. For the UK sample, the amount of investment had a marginal effect but prompt timing had an interesting impact. Unlike the British sample, the GenAI application, prompt timing and investment amount did not significantly influence the Bulgarian respondents’ evaluations. While the mean responses of the British sample were slightly higher, these differences were not statistically significant, indicating that ChatGPT, Bing and Bard performed similarly in both the UK and Bulgaria.
Originality/value
The study assesses the relevance, accuracy, specificity and justification of GenAI chatbots’ investment recommendations for two different periods, investment amounts and countries.}
}
@article{YE2024102851,
title = {Privacy and personal data risk governance for generative artificial intelligence: A Chinese perspective},
journal = {Telecommunications Policy},
volume = {48},
number = {10},
pages = {102851},
year = {2024},
issn = {0308-5961},
doi = {https://doi.org/10.1016/j.telpol.2024.102851},
url = {https://www.sciencedirect.com/science/article/pii/S0308596124001484},
author = {Xiongbiao Ye and Yuhong Yan and Jia Li and Bo Jiang},
keywords = {Generative AI, Privacy and personal data protection, Risk governance, Chinese law},
abstract = {The rapid development of generative artificial intelligence (AI) has attracted global attention and posed challenges to existing data governance frameworks. The increased technical complexity and expanded scale of data usage not only make it more difficult to regulate AI but also present challenges for the current legal system. This article, which takes ChatGPT's training data and working principles as a starting point, examines specific privacy risks, data leakage risks, and personal data risks posed by generative AI. It also analyzes the latest practices in privacy and personal data protection in China. This article finds that while China's governance on privacy and personal data protection takes a macro-micro integration approach and a private-and-public law integration approach, there are shortcomings in the legal system. Given that the current personal data protection system centered on individual control is unsuitable for the modes of data processing by generative AI, and that private law is insufficient in safeguarding data privacy, urgent institutional innovation is needed to achieve the objective of “trustworthy AI.”}
}
@article{JIN2025105248,
title = {High heels, compass, spider-man, or drug? Metaphor analysis of generative artificial intelligence in academic writing},
journal = {Computers & Education},
volume = {228},
pages = {105248},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105248},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000168},
author = {Fangzhou Jin and Lanfang Sun and Yunqiu Pan and Chin-Hsi Lin},
keywords = {Artificial intelligence, Writing, Metaphor analysis, Cross-cultural skills, Interdisciplinary knowledge},
abstract = {This research employed metaphor analysis to explore 277 postgraduate students' perceptions of the role of generative artificial intelligence (GenAI) in academic writing. All participants were international students, from a total of 14 countries and regions, studying in the United Kingdom. Data collection was carried out in two phases. The first was a survey comprising demographic and metaphor-related questions, and the second involved metaphor checking, in which participants provided screenshots of their interactions with GenAI. The data, which were analyzed both qualitatively and quantitatively, yielded 53 unique metaphors for the concept of GenAI in academic writing. We divided these into four conceptual categories in what we term the 4T Pyramid Model: Technical Support (representative metaphor: high-heeled shoes), Text Development (compass), Transformative Potential (Spider-Man), and Threat (drug). The respondents' academic disciplines influenced their perceptions of GenAI, but overall, the results suggest that most viewed it as transformative, i.e., more than just a writing tool. This study's innovative methodology integrating metaphor analysis with real user interactions offers a framework, aligned with Bloom's Taxonomy, that reveals the multi-level benefits and potential risks of GenAI. It also provides actionable insights for AI literacy education, including strategies for effective prompt design.}
}
@article{CRUMBLY2025103129,
title = {A classification framework for generative artificial intelligence for social good},
journal = {Technovation},
volume = {139},
pages = {103129},
year = {2025},
issn = {0166-4972},
doi = {https://doi.org/10.1016/j.technovation.2024.103129},
url = {https://www.sciencedirect.com/science/article/pii/S0166497224001792},
author = {Jack Crumbly and Raktim Pal and Nezih Altay},
keywords = {Artificial intelligence (AI), Generative artificial intelligence (GenAI), Social good, Classification},
abstract = {Many policy makers and corporate leaders are adjusting their strategies to harness the power of GenAI. There are numerous debates on how GenAI would fundamentally change existing business models. However, there is not much discussion on roles of generative AI in the domain of social good. Broader views covering potential opportunities of GenAI to enable diverse initiatives in the social good space are largely missing. We intend to reduce the gap by developing a classification framework that should allow researchers gauge the potential impact of GenAI for social good initiatives. Through case analysis, we assess how value-added abilities of GenAI may influence various social good initiatives. We adopt/develop two loosely connected classification frameworks that are grounded in task-technology fit (TTF) theory. Subsequently, we investigate how our analyses of GenAI initiatives utilizing different dimensions of these two frameworks may be synthesized to provide appropriate explanation for potential success of GenAI for social good. We develop five propositions that will provide guidance to practitioners and researchers. The theoretically grounded analysis of 21 GenAI for social good use cases based on the two classification frameworks, and the resulting propositions are the original contributions of this paper to the AI for social good literature.}
}
@article{CALLARI2025266,
title = {Can generative artificial intelligence productivity tools support workplace learning? A qualitative study on employee perceptions in a multinational corporation},
journal = {Journal of Workplace Learning},
volume = {37},
number = {3},
pages = {266-283},
year = {2025},
issn = {1366-5626},
doi = {https://doi.org/10.1108/JWL-11-2024-0258},
url = {https://www.sciencedirect.com/science/article/pii/S1366562625000026},
author = {Tiziana C. Callari and Lucia Puppione},
keywords = {Organisational socialisation, Meaningful work, Formal and informal learning, Incidental learning, Sociotechnical capital},
abstract = {Purpose
The purpose of this study was to explore employees’ perceptions and firsthand experiences of the impact of generative artificial intelligence (AI) productivity tools, specifically Microsoft 365 Copilot, on individual and collective learning processes within a multinational corporation. In doing so, the study provides insights into how these tools can shape workplace learning dynamics, fostering both individual skill development and collaborative knowledge-sharing practices.
Design/methodology/approach
The authors collected responses from 357 participants through a survey that included both multiple-choice and open-ended questions. This study focuses exclusively on the qualitative responses. The reflexive thematic analysis method was used to capture and interpret employees’ perceptions of the role of Microsoft 365 Copilot – a generative AI-powered assistant integrated into the Microsoft 365 suite of applications (e.g., Word, Excel, PowerPoint, Outlook, Teams) – in enhancing their work and learning opportunities in the workplace.
Findings
The results highlight four key themes contributing to workplace learning. At the individual level, Task Support illustrates the extent to which generative AI productivity tools transform work practices and facilitate both formal and informal learning pathways, while Meaningful Work underscores the tools’ role in enhancing employees’ foundational knowledge through enriched information. At the organisational level, organisational culture suggests the importance of fostering a supportive environment for AI integration, while organisational socialisation highlights its influence on team cohesion and the informal knowledge-sharing processes essential for effective collaboration within and among team members.
Practical implications
The results of this study offer actionable insights for organisations integrating generative AI productivity tools in the workplace. Understanding employees’ perceptions of the role of AI in workplace learning can inform the design of targeted training programmes that promote individual skill development and foster collaborative knowledge sharing. Furthermore, a supportive organisational culture that positions AI as a complementary resource can improve employee engagement, reduce resistance to new technologies and encourage a growth-oriented mindset, ultimately driving both personal and organisational development.
Originality/value
This study shifts the narrative around the role of AI in the workplace by examining how generative AI productivity tools can enhance workplace learning at both individual and organisational levels, rather than focusing solely on their potential to disrupt work through displacement and automation. By positioning AI-based applications as complementary to human work, this approach highlights their potential as enablers of skill development, knowledge sharing and job enrichment, fostering a more adaptive and learning-oriented work environment.}
}
@article{NING2024e848,
title = {Generative artificial intelligence and ethical considerations in health care: a scoping review and ethics checklist},
journal = {The Lancet Digital Health},
volume = {6},
number = {11},
pages = {e848-e856},
year = {2024},
issn = {2589-7500},
doi = {https://doi.org/10.1016/S2589-7500(24)00143-2},
url = {https://www.sciencedirect.com/science/article/pii/S2589750024001432},
author = {Yilin Ning and Salinelat Teixayavong and Yuqing Shang and Julian Savulescu and Vaishaanth Nagaraj and Di Miao and Mayli Mertens and Daniel Shu Wei Ting and Jasmine Chiat Ling Ong and Mingxuan Liu and Jiuwen Cao and Michael Dunn and Roger Vaughan and Marcus Eng Hock Ong and Joseph Jao-Yiu Sung and Eric J Topol and Nan Liu},
abstract = {Summary
The widespread use of Chat Generative Pre-trained Transformer (known as ChatGPT) and other emerging technology that is powered by generative artificial intelligence (GenAI) has drawn attention to the potential ethical issues they can cause, especially in high-stakes applications such as health care, but ethical discussions have not yet been translated into operationalisable solutions. Furthermore, ongoing ethical discussions often neglect other types of GenAI that have been used to synthesise data (eg, images) for research and practical purposes, which resolve some ethical issues and expose others. We did a scoping review of the ethical discussions on GenAI in health care to comprehensively analyse gaps in the research. To reduce the gaps, we have developed a checklist for comprehensive assessment and evaluation of ethical discussions in GenAI research. The checklist can be integrated into peer review and publication systems to enhance GenAI research and might be useful for ethics-related disclosures for GenAI-powered products and health-care applications of such products and beyond.}
}
@article{SUPPAN2025,
title = {Performance of 3 Conversational Generative Artificial Intelligence Models for Computing Maximum Safe Doses of Local Anesthetics: Comparative Analysis},
journal = {JMIR AI},
volume = {4},
year = {2025},
issn = {2817-1705},
doi = {https://doi.org/10.2196/66796},
url = {https://www.sciencedirect.com/science/article/pii/S2817170525000419},
author = {Mélanie Suppan and Pietro Elias Fubini and Alexandra Stefani and Mia Gisselbaek and Caroline Flora Samer and Georges Louis Savoldelli},
keywords = {local anesthetic, dose calculation, toxicity, performance, conversational generative artificial intelligence, artificial intelligence, anesthesiology, comparative analysis, anesthetics, LA, generative artificial intelligence, ChatGPT, Copilot, Gemini, artificial intelligence models, machine learning, neural network, LLM, NLP, natural language processing, large language model, AI, ML},
abstract = {Background
Generative artificial intelligence (AI) is showing great promise as a tool to optimize decision-making across various fields, including medicine. In anesthesiology, accurately calculating maximum safe doses of local anesthetics (LAs) is crucial to prevent complications such as local anesthetic systemic toxicity (LAST). Current methods for determining LA dosage are largely based on empirical guidelines and clinician experience, which can result in significant variability and dosing errors. AI models may offer a solution, by processing multiple parameters simultaneously to suggest adequate LA doses.
Objective
This study aimed to evaluate the efficacy and safety of 3 generative AI models, ChatGPT (OpenAI), Copilot (Microsoft Corporation), and Gemini (Google LLC), in calculating maximum safe LA doses, with the goal of determining their potential use in clinical practice.
Methods
A comparative analysis was conducted using a 51-item questionnaire designed to assess LA dose calculation across 10 simulated clinical vignettes. The responses generated by ChatGPT, Copilot, and Gemini were compared with reference doses calculated using a scientifically validated set of rules. Quantitative evaluations involved comparing AI-generated doses to these reference doses, while qualitative assessments were conducted by independent reviewers using a 5-point Likert scale.
Results
All 3 AI models (Gemini, ChatGPT, and Copilot) completed the questionnaire and generated responses aligned with LA dose calculation principles, but their performance in providing safe doses varied significantly. Gemini frequently avoided proposing any specific dose, instead recommending consultation with a specialist. When it did provide dose ranges, they often exceeded safe limits by 140% (SD 103%) in cases involving mixtures. ChatGPT provided unsafe doses in 90% (9/10) of cases, exceeding safe limits by 198% (SD 196%). Copilot’s recommendations were unsafe in 67% (6/9) of cases, exceeding limits by 217% (SD 239%). Qualitative assessments rated Gemini as “fair” and both ChatGPT and Copilot as “poor.”
Conclusions
Generative AI models like Gemini, ChatGPT, and Copilot currently lack the accuracy and reliability needed for safe LA dose calculation. Their poor performance suggests that they should not be used as decision-making tools for this purpose. Until more reliable AI-driven solutions are developed and validated, clinicians should rely on their expertise, experience, and a careful assessment of individual patient factors to guide LA dosing and ensure patient safety.}
}
@article{SILALAHI2025102995,
title = {Can generative artificial intelligence drive sustainable behavior? A consumer-adoption model for AI-driven sustainability recommendations},
journal = {Technology in Society},
volume = {83},
pages = {102995},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.102995},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X2500185X},
author = {Andri Dayarana K. Silalahi},
keywords = {Generative AI, Sustainable behavior, User trust, Elaboration likelihood model, Cognitive engagement, Pro-environmental adoption},
abstract = {Generative AI (GAI) has the potential to promote sustainable behavior through personalized recommendations; yet its effectiveness hinges on user trust—an issue that remains under-explored in the literature. Existing studies often focus on specific domains without addressing broader trust-building mechanisms or the cognitive and motivational factors needed for sustained engagement. This study investigates how trust shapes the adoption of GAI-driven sustainability recommendations by integrating the Elaboration Likelihood Model (ELM) and Expectancy-Value Theory (EVT) into a single framework. Using data from sustainability-oriented users, we examine how central route constructs-perceived information quality and utility-peripheral route constructs-anthropomorphism and interaction quality-enhance trust, while perceived information complexity and perceived risk moderate these relationships. Our findings indicate that high-quality, useful information enhances trust through cognitive engagement, whereas anthropomorphic design and interaction quality reinforce trust via the heuristic route. However, excessive complexity and privacy concerns undermine trust, highlighting the need for clearer communication and data transparency. This study broadens theoretical understanding by extending ELM and EVT to the context of GAI-driven sustainability efforts, providing an integrated framework that encompasses cognitive and motivational trust drivers. These insights fill gaps in technology adoption research and offer practical guidance for developing GAI platforms that effectively support pro-environmental behavior change.}
}
@article{HUANG2025100526,
title = {Generative spatial artificial intelligence for sustainable smart cities: A pioneering large flow model for urban digital twin},
journal = {Environmental Science and Ecotechnology},
volume = {24},
pages = {100526},
year = {2025},
issn = {2666-4984},
doi = {https://doi.org/10.1016/j.ese.2025.100526},
url = {https://www.sciencedirect.com/science/article/pii/S2666498425000043},
author = {Jeffrey Huang and Simon Elias Bibri and Paul Keel},
keywords = {Sustainable smart cities, Generative artificial intelligence, Generative spatial artificial intelligence, Foundation models, Large flow model, Urban digital twin, Urban planning and design},
abstract = {Rapid urbanization, alongside escalating resource depletion and ecological degradation, underscores the critical need for innovative urban development solutions. In response, sustainable smart cities are increasingly turning to cutting-edge technologies—such as Generative Artificial Intelligence (GenAI), Foundation Models (FMs), and Urban Digital Twin (UDT) frameworks—to transform urban planning and design practices. These transformative tools provide advanced capabilities to analyze complex urban systems, optimize resource management, and enable evidence-based decision-making. Despite recent progress, research on integrating GenAI and FMs into UDT frameworks remains scant, leaving gaps in our ability to capture complex urban flows and multimodal dynamics essential to achieving environmental sustainability goals. Moreover, the lack of a robust theoretical foundation and real-world operationalization of these tools hampers comprehensive modeling and practical adoption. This study introduces a pioneering Large Flow Model (LFM), grounded in a robust foundational framework and designed with GenAI capabilities. It is specifically tailored for integration into UDT systems to enhance predictive analytics, adaptive learning, and complex data management functionalities. To validate its applicability and relevance, the Blue City Project in Lausanne City is examined as a case study, showcasing the ability of the LFM to effectively model and analyze urban flows—namely mobility, goods, energy, waste, materials, and biodiversity—critical to advancing environmental sustainability. This study highlights how the LFM addresses the spatial challenges inherent in current UDT frameworks. The LFM demonstrates its novelty in comprehensive urban modeling and analysis by completing impartial city data, estimating flow data in new locations, predicting the evolution of flow data, and offering a holistic understanding of urban dynamics and their interconnections. The model enhances decision-making processes, supports evidence-based planning and design, fosters integrated development strategies, and enables the development of more efficient, resilient, and sustainable urban environments. This research advances both the theoretical and practical dimensions of AI-driven, environmentally sustainable urban development by operationalizing GenAI and FMs within UDT frameworks. It provides sophisticated tools and valuable insights for urban planners, designers, policymakers, and researchers to address the complexities of modern cities and accelerate the transition towards sustainable urban futures.}
}
@article{LEE2025104317,
title = {Readability, quality and accuracy of generative artificial intelligence chatbots for commonly asked questions about labor epidurals: a comparison of ChatGPT and Bard},
journal = {International Journal of Obstetric Anesthesia},
volume = {61},
pages = {104317},
year = {2025},
issn = {0959-289X},
doi = {https://doi.org/10.1016/j.ijoa.2024.104317},
url = {https://www.sciencedirect.com/science/article/pii/S0959289X24003297},
author = {D. Lee and M. Brown and J. Hammond and M. Zakowski},
keywords = {Labor analgesia, Epidural, Pregnancy, Generative Artificial Intelligence, Patient educational materials},
abstract = {Introduction
Over 90% of pregnant women and 76% expectant fathers search for pregnancy health information. We examined readability, accuracy and quality of answers to common obstetric anesthesia questions from the popular generative artificial intelligence (AI) chatbots ChatGPT and Bard.
Methods
Twenty questions for generative AI chatbots were derived from frequently asked questions based on professional society, hospital and consumer websites. ChatGPT and Bard were queried in November 2023. Answers were graded for accuracy by four obstetric anesthesiologists. Quality was measured using Patient Education Materials Assessment Tool for Print (PEMAT). Readability was measured using six readability indices. Accuracy, quality and readability were compared using independent t-test.
Results
Bard readability scores were high school level, significantly easier than ChatGPT’s college level by all scoring metrics (P <0.001). Bard had significantly longer answers (P <0.001), yet with similar accuracy of Bard (85% ± 10) and ChatGPT (87% ± 14) (P=0.5). PEMAT understandability scores were no statistically significantly different (P=0.06). Actionability by PEMAT scores for Bard was significantly higher (22% vs. 9%) than ChatGPT (P=0.007)
Conclusion
Answers to questions about “labor epidurals” should be accurate, high quality, and easy to read. Bard at high school reading level, was well above the goal 4th to 6th grade level suggested for patient materials. Consumers, health care providers, hospitals and governmental agencies should be aware of the quality of information generated by chatbots. Chatbots should meet the standards for readability and understandability of health-related questions, to aid public understanding and enhance shared decision-making.}
}
@article{MARZOUK2025118228,
title = {Editorial: Generative Artificial Intelligence for Predictive Simulations and Decision-Making in Science and Engineering},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {445},
pages = {118228},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.118228},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525005006},
author = {Youssef Marzouk and Benjamin Peherstorfer}
}
@article{ALKHATIB2024102676,
title = {How can generative artificial intelligence improve digital supply chain performance in manufacturing firms? Analyzing the mediating role of innovation ambidexterity using hybrid analysis through CB-SEM and PLS-SEM},
journal = {Technology in Society},
volume = {78},
pages = {102676},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102676},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24002240},
author = {Ayman wael Al-khatib and Moh'd Anwer AL-Shboul and Mais Khattab},
keywords = {Generative artificial intelligence, Innovation ambidexterity, Digital supply chain, Manufacturing firms, Performance, Hybrid analysis, Jordan},
abstract = {Artificial intelligence capabilities (AIC) can influence supply chain management (SCM) in multiple ways. This study explores how generative artificial intelligence capabilities (GAIC) could affect digital supply chain performance (DSCP) through ambidexterity innovation (AMI), which includes both elements, exploratory and exploitative innovations in the manufacturing firms (MFs) in Jordan as a developing and emerging economy. This study adopted a quantitative methodology for the data collection process applying a cross-sectional approach through testing deductive-hypotheses techniques. 263 valid surveys were used for analysis using hybrid analysis measurements (i.e., PLS-SEM, and CB-SEM). Further, it was applied data reliability, convergent validity, and discriminant validity tests. Additionally, examined the mediating effect of exploratory innovation (EXPI), and exploitative innovation (EXTI) on DSCP. The study findings assured that the proposed direct and indirect causal associations illustrated in the study model were accepted due to that all associations between the dimensions s were statistically significant. The findings of the GAIC supported a positive relationship between GAIC and the DSCP, GAIC on EXPI and EXTI, and EXPI and EXTI on DSCP respectively. Furthermore, the mediating effect of EXPI and EXTI is statistically significant, which was confirmed. This study developed a conceptual model to merge GAIC, AMI, and DSCP. This study provides new outcomes that bridge the existing research gap in the literature by testing the mediation model with a focus on the MF benefits of GAIC to improve levels of EXPI, EXTI, and DSCP in Jordan as a developing and emerging economy. Furthermore, this study is considered unique, as it was the first study in Jordan, and through applying hybrid analysis measurements using both PLS-SEM and CB-SEM methods.}
}
@article{WHEATLEY2024102942,
title = {Comparing generative artificial intelligence tools to voice assistants using reference interactions},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {5},
pages = {102942},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102942},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324001034},
author = {Amanda Wheatley and Sandy Hervieux},
keywords = {Artificial intelligence, Voice assistants, Generative AI, Reference},
abstract = {This study investigates the ability of voice assistants and generative AI tools to respond to reference questions traditionally received by academic librarians. The authors created a sample of 25 questions based on queries received on the virtual reference service at their institution. They then created a rubric to evaluate the quality of the answers that the AI powered tools provided. The authors determined that the tools understand reference questions well and provide relevant answers but that the quality of the references provided, and the accuracy of the answers can be lacking. They suggest that more research needs to be done to understand the place of AI powered tools in reference services.}
}
@article{SALAH2024101872,
title = {The good, the bad, and the GPT: Reviewing the impact of generative artificial intelligence on psychology},
journal = {Current Opinion in Psychology},
volume = {59},
pages = {101872},
year = {2024},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2024.101872},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X2400085X},
author = {Mohammed Salah and Fadi Abdelfattah and Hussam {Al Halbusi}},
keywords = {Generative artificial intelligence, Psychology, Ethical considerations, Therapeutic personalization, Natural language processing},
abstract = {This review explores the impact of Generative Artificial Intelligence (GenAI)—a technology capable of autonomously creating new content, ideas, or solutions by learning from extensive data—on psychology. GenAI is changing research methodologies, diagnostics, and treatments by enhancing diagnostic accuracy, personalizing therapeutic interventions, and providing deeper insights into cognitive processes. However, these advancements come with significant ethical concerns, including privacy, bias, and the risk of depersonalization in therapy. By focusing on the current capabilities of GenAI, this study aims to provide a balanced understanding and guide the ethical integration of AI into psychological practices and research. We argue that while GenAI presents profound opportunities, its integration must be approached cautiously using robust ethical frameworks.}
}
@article{MOTOKI2025106904,
title = {Assessing political bias and value misalignment in generative artificial intelligence},
journal = {Journal of Economic Behavior & Organization},
volume = {234},
pages = {106904},
year = {2025},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2025.106904},
url = {https://www.sciencedirect.com/science/article/pii/S0167268125000241},
author = {Fabio Y.S. Motoki and Valdemar {Pinho Neto} and Victor Rangel},
keywords = {Generative AI, Societal values, Large language models, Multimodal, AI governance},
abstract = {Our analysis reveals a concerning misalignment of values between ChatGPT and the average American. We also show that ChatGPT displays political leanings when generating text and images, but the degree and direction of skew depend on the theme. Notably, ChatGPT repeatedly refused to generate content representing certain mainstream perspectives, citing concerns over misinformation and bias. As generative AI systems like ChatGPT become ubiquitous, such misalignment with societal norms poses risks of distorting public discourse. Without proper safeguards, these systems threaten to exacerbate societal divides and depart from principles that underpin free societies.}
}
@article{MARTIKAINEN2025,
title = {Evaluation of Generative Artificial Intelligence Implementation Impacts in Social and Health Care Language Translation: Mixed Methods Case Study},
journal = {JMIR Formative Research},
volume = {9},
year = {2025},
issn = {2561-326X},
doi = {https://doi.org/10.2196/73658},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X25006420},
author = {Miia Martikainen and Kari Smolander and Johan Sanmark and Enni Sanmark},
keywords = {generative artificial intelligence, large language model, ChatGPT, pretrained language model, language translation, machine translation evaluation, public social and health care},
abstract = {Background
Generative artificial intelligence (GAI) is expected to enhance the productivity of the public social and health care sector while maintaining, at minimum, current standards of quality and user experience. However, empirical evidence on GAI impacts in practical, real-life settings remains limited.
Objective
This study investigates productivity, machine translation quality, and user experience impacts of the GPT-4 language model in an in-house language translation services team of a large well-being services county in Finland.
Methods
A mixed methods study was conducted with 4 in-house translators between March and June 2024. Quantitative data of 908 translation segments were collected in real-life conditions using the computer-assisted language translation software Trados (RWS) to assess productivity differences between machine and human translation. Quality was measured using 4 automatic metrics (human-targeted translation edit rate, Bilingual Evaluation Understudy, Metric for Evaluation of Translation With Explicit Ordering, and Character n-gram F-score) applied to 1373 GAI-human segment pairs. User experience was investigated through 5 semistructured interviews, including the team supervisor.
Results
The findings indicate that, on average, postediting machine translation is 14% faster than translating texts from scratch (2.75 vs 2.40 characters per second, P=.03), and up to 37% faster when the number of segments is equalized across translators. However, productivity varied notably between individuals, with improvements ranging from −2% to 102%. Regarding translation quality, 11% (141/1261) of Finnish-Swedish and 16% (18/112) of Finnish-English GAI outputs were accepted without edits. Average human-targeted translation edit rate scores were 55 (Swedish) and 46 (English), indicating that approximately half of the words required editing. Bilingual Evaluation Understudy scores averaged 43 for Swedish and 38 for English, suggesting good translation quality. Metric for Evaluation of Translation With Explicit Ordering and Character n-gram F-scores reached 63 and 68 for Swedish and 59 and 57 for English, respectively. All metrics have been converted to an equivalent scale from 0 to 100, with 100 reflecting a perfect match. Interviewed translators expressed mixed reviews on productivity gains but generally perceived value in using GAI, especially for repetitive, generic content. Identified challenges included inconsistent or incorrect terminology, lack of document-level context, and limited system customization.
Conclusions
Based on this case study, GPT-4–based GAI shows measurable potential to enhance translation productivity and quality within an in-house translation team in the public social and health care sector. However, its effectiveness appears to be influenced by factors, such as translator postediting skills, workflow design, and organizational readiness. These findings suggest that, in similar contexts, public social and health care organizations could benefit from investing in translator training, optimizing technical integration, redesigning workflows, and implementing effective change management. Future research should examine larger translator teams to assess the generalizability of these results and further explore how translation quality and user experience can be improved through domain-specific customization.}
}
@article{BUI2025101392,
title = {Exploring value co-creation and co-destruction between consumers & generative artificial intelligence (GAI) in travel},
journal = {Tourism Management Perspectives},
volume = {58},
pages = {101392},
year = {2025},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2025.101392},
url = {https://www.sciencedirect.com/science/article/pii/S2211973625000571},
author = {Hien Thu Bui and Viachaslau Filimonau and Hakan Sezerel},
keywords = {Emerging technology, Generative artificial intelligence, Travel assistance, Value co-creation, Value co-destruction, ChatGPT},
abstract = {Little is known about the (dis)benefits of using generative artificial intelligence (GAI) with travel-related purposes, which hinders an understanding of the value co-created and co-destructed in the process of its use by tourists. This mixed methods study explored and examined the key factors in value co-creation and co-destruction when using a popular GAI's conversational interface, ChatGPT, in tourism. The results indicate that the key perceived utility of ChatGPT is in travel planning and time saving, and the main perceived shortcomings are its limited knowledge and inaccurate responses. The study pinpoints the importance of refining and developing GAI collaboratively by all tourism stakeholders given that perceived value co-creation outweighs value co-destruction.}
}
@article{LUO2025e117,
title = {Transparent reporting of generative artificial intelligence use in systematic reviews},
journal = {Journal of the American Academy of Dermatology},
volume = {93},
number = {3},
pages = {e117-e118},
year = {2025},
issn = {0190-9622},
doi = {https://doi.org/10.1016/j.jaad.2025.03.101},
url = {https://www.sciencedirect.com/science/article/pii/S0190962225022078},
author = {Xufei Luo and Yaolong Chen},
keywords = {cutaneous squamous cell carcinoma, generative AI, reporting guideline, systematic review}
}
@article{KAPLAN2025106080,
title = {New generative artificial intelligence model: ScholarGPT’s performance on dental avulsion},
journal = {International Journal of Medical Informatics},
volume = {204},
pages = {106080},
year = {2025},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2025.106080},
url = {https://www.sciencedirect.com/science/article/pii/S1386505625002977},
author = {Taibe Tokgöz Kaplan},
keywords = {Artificial intelligence, ChatGPT, Dental avulsion, Gemini, Large Language Models, ScholarGPT},
abstract = {Background
This study aims to evaluate the performance of ScholarGPT, a Large Language Model (LLM) developed for academic purposes, on questions related to dental avulsion. In addition, to analyze and compare it with the results of the previous study evaluating the performance of ChatGPT and Gemini.
Method
A total of 22 technical questions (11 multiple-choice questions (MCQs), 11 true/false (T/F)) were posed to the ScholarGPT. ScholarGPT responses were assessed using a modified Global Quality Scale (GQS). Responses were randomized using an online randomizer (www.randomizer.org) before scoring. A single researcher carried out the assessments at three different times, two weeks apart, and a new randomization was performed before each scoring.
Results
When the answers given by ScholarGPT according to the question groups were analyzed by the Mann-Whitney U test, the mean value was found to be 4.64 for MCQ questions and 4.82 for T/F questions. ScholarGPT provided similarly high-quality and consistent answers in both question types (p = 0.590). When the performance of ScholarGPT was compared with Gemini and ChatGPT via the Friedman test, the mean score of ScholarGPT responses was significantly higher than both ChatGPT and Gemini (mean difference with Gemini = 0.75; mean difference with ChatGPT = 1.62, p < 0.001). ScholarGPT produced statistically significantly more consistent and higher-quality responses than ChatGPT and Gemini.
Conclusion
ScholarGPT showed high performance on technical questions related to dental avulsion and produced more consistent and higher-quality answers than ChatGPT and Gemini. According to the findings, LLMs based on academic databases can provide more accurate and reliable information. In the future, through the development of LLMs specific to the branches of dentistry, artificial intelligence systems can produce higher quality and consistent information.}
}
@article{CHEAH2025100363,
title = {Integrating generative artificial intelligence in K-12 education: Examining teachers’ preparedness, practices, and barriers},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100363},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000037},
author = {Yin Hong Cheah and Jingru Lu and Juhee Kim},
keywords = {Generative artificial intelligence, In-service teachers, Preparedness, Practices, Barriers, K-12 education},
abstract = {Despite the growing body of research on developing K-12 teachers' generative AI (GenAI) knowledge and skills, its integration into daily teaching practices remains underexplored. Using a snowball sampling method, this study examined the preparedness, practices, and barriers encountered by 89 U.S. teachers in the state of Idaho. Participants were predominantly White, female teachers serving in rural schools. A mixed-methods analysis of survey responses revealed that teachers were generally underprepared for integrating GenAI, with fewer than half incorporating it into their educational practices. Unlike the widespread classroom integration patterns observed with general educational technologies, teachers in this study tended to use GenAI for out-of-classroom duties (i.e., lesson preparation, assessment, and administrative tasks) rather than for real-time teaching and learning. These preferences could be attributed to key barriers teachers faced, including doubts about GenAI's ability to manage risks (i.e., technology value beliefs), reduced human interaction in instruction (i.e., pedagogical beliefs), ethical considerations, and the absence of policies and guidance. This study highlights the need to develop support systems and targeted policies to facilitate teachers' GenAI integration, offering implications for Idaho's education system and the broader U.S. context.}
}
@article{CAMPELLONE2025,
title = {Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/67365},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125007381},
author = {Timothy R Campellone and Megan Flom and Robert M Montgomery and Lauren Bullard and Maddison C Pirner and Aaron Pavez and Michelle Morales and Devin Harper and Catherine Oddy and Tom O'Connor and Jade Daniels and Stephanie Eaneff and Valerie L Forman-Hoffman and Casey Sackett and Alison Darcy},
keywords = {generative AI, digital mental health intervention, user experience, RCT, randomized, controlled trials, randomized controlled trial, chatbots, artificial intelligence, AI, user relationship, user satisfaction, user safety, user, exploratory, relationship, satisfaction, safety, generative, DMHI, mental health, digital health},
abstract = {Background
General awareness and exposure to generative artificial intelligence (AI) have increased recently. This transformative technology has the potential to create a more dynamic and engaging user experience in digital mental health interventions (DMHIs). However, if not appropriately used and controlled, it can introduce risks to users that may result in harm and erode trust. At the time of conducting this trial, there had not been a rigorous evaluation of an approach to safely implementing generative AI in a DMHI.
Objective
This study aims to explore the user relationship, experience, safety, and technical guardrails of a DMHI using generative AI compared with a rules-based intervention.
Methods
We conducted a 2-week exploratory randomized controlled trial (RCT) with 160 adult participants randomized to receive a generative AI (n=81) or rules-based (n=79) version of a conversation-based DMHI. Self-report measures of the user relationship (client satisfaction, working alliance bond, and accuracy of empathic listening and reflection) and experience (engagement metrics, adverse events, and technical guardrail success) were collected. Descriptions and validation of technical guardrails for handling user inputs (eg, detecting potentially concerning language and off-topic responses) and model outputs (eg, not providing medical advice and not providing a diagnosis) are provided, along with examples to illustrate how they worked. Safety monitoring was conducted throughout the trial for adverse events, and the success of technical guardrails created for the generative arm was assessed post trial.
Results
In general, the majority of measures of user relationship and experience appeared to be similar in both the generative and rules-based arms. The generative arm appeared to be more accurate at detecting and responding to user statements with empathy (98% accuracy vs 69%). There were no serious or device-related adverse events, and technical guardrails were shown to be 100% successful in posttrial review of generated statements. A majority of participants in both groups reported an increase in positive sentiment (62% and 66%) about AI at the end of the trial.
Conclusions
This trial provides initial evidence that, with the right guardrails and process, generative AI can be successfully used in a digital mental health intervention (DMHI) while maintaining the user experience and relationship. It also provides an initial blueprint for approaches to technical and conversational guardrails that can be replicated to build a safe DMHI.
Trial Registration
ClinicalTrials.gov NCT05948670; https://clinicaltrials.gov/study/NCT05948670}
}
@article{SOLORZANOREQUEJO2024102157,
title = {Fostering creativity in engineering design through constructive dialogues with generative artificial intelligence},
journal = {Cell Reports Physical Science},
volume = {5},
number = {9},
pages = {102157},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.102157},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424004429},
author = {William {Solórzano Requejo} and Francisco {Franco Martínez} and Carlos {Aguilar Vega} and Rodrigo {Zapata Martínez} and Adrián {Martínez Cendrero} and Andrés {Díaz Lantada}},
keywords = {artificial intelligence, engineering design, creativity promotion, biohybrid materials, medical devices, product design, architected materials: architectural structures},
abstract = {Summary
Artificial intelligence (AI) is progressively reshaping the way that researchers design and study highly complex systems. In this perspective, we introduce an engineering design methodology aimed at fostering creativity through “constructive dialogues with a generative AI” and exemplify its potential through a set of methodically developed case studies. This creativity promotion approach starts with computer-aided design (CAD) models of lattices, metamaterials, and architected materials, which are provided as initial inputs to a generative AI through a chat. Then, the conversation starts with researchers asking the generative AI to modify the provided CAD model images by incorporating new elements, placing them in quasi-real-life environments, or adapting the provided designs to the structures of new products. To illustrate the methodology, a varied set of selected case studies of constructive dialogues leading to highly innovative designs are provided, bridging the gap between tissue engineering scaffolds and building architectures, biohybrid materials and product design, and innovative structures and medical devices, to cite a few.}
}
@article{BARAKCORREN2025102242,
title = {From Text to Data: Automatically Extracting Data From Catheterization Reports Using Generative Artificial Intelligence},
journal = {Journal of the Society for Cardiovascular Angiography & Interventions},
volume = {4},
number = {3, Part B},
pages = {102242},
year = {2025},
note = {The Role of Artificial Intelligence in Cardiovascular Interventions},
issn = {2772-9303},
doi = {https://doi.org/10.1016/j.jscai.2024.102242},
url = {https://www.sciencedirect.com/science/article/pii/S2772930324016247},
author = {Yuval Barak-Corren and Mudit Gupta and Jessica Tang and Christopher L. Smith and Ryan Callahan and Yoav Dori and Jonathan J. Rome and Matthew J. Gillespie and Michael L. O’Byrne},
keywords = {artificial intelligence, generative artificial intelligence, health informatics, interventional cardiology, natural language processing, pediatric cardiology}
}
@article{MESSER2025100108,
title = {How do people react to political bias in generative artificial intelligence (AI)?},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {3},
pages = {100108},
year = {2025},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2024.100108},
url = {https://www.sciencedirect.com/science/article/pii/S2949882124000689},
author = {Uwe Messer},
keywords = {Artificial intelligence, Alignment, Political orientation, Bias, Acceptance, Large language model},
abstract = {Generative Artificial Intelligence (GAI) such as Large Language Models (LLMs) have a concerning tendency to generate politically biased content. This is a challenge, as the emergence of GAI meets politically polarized societies. Therefore, this research investigates how people react to biased GAI-content based on their pre-existing political beliefs and how this influences the acceptance of GAI. In three experiments (N = 513), it was found that perceived alignment between user's political orientation and bias in generated content (in text and images) increases acceptance and reliance on GAI. Participants who perceived alignment were more likely to grant GAI access to sensitive smartphone functions and to endorse the use in critical domains (e.g., loan approval; social media moderation). Because users see GAI as a social actor, they consider perceived alignment as a sign of greater objectivity, thus granting aligned GAI access to more sensitive areas.}
}
@article{CURRIE2025103,
title = {Gender bias in text-to-image generative artificial intelligence depiction of Australian paramedics and first responders},
journal = {Australasian Emergency Care},
volume = {28},
number = {2},
pages = {103-109},
year = {2025},
issn = {2588-994X},
doi = {https://doi.org/10.1016/j.auec.2024.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2588994X24000757},
author = {Geoffrey Currie and Johnathan Hewis and Phillip Ebbs},
keywords = {First responder, Generative artificial intelligence, Diversity, Inclusivity},
abstract = {Introduction
In Australia, almost 50 % of paramedics are female yet they remain under-represented in stereotypical depictions of the profession. The potentially transformative value of generative artificial intelligence (AI) may be limited by stereotypical errors, misrepresentations and bias. Increasing use of text-to-image generative AI, like DALL-E 3, could reinforce gender and ethnicity biases and, therefore, is important to objectively evaluate.
Method
In March 2024, DALL-E 3 was utilised via GPT-4 to generate a series of individual and group images of Australian paramedics, ambulance officers, police officers and firefighters. In total, 82 images were produced including 60 individual-character images, and 22 multiple-character group images. All 326 depicted characters were independently analysed by three reviewers for apparent gender, age, skin tone and ethnicity.
Results
Among first responders, 90.8 % (N = 296) were depicted as male, 90.5 % (N = 295) as Caucasian, 95.7 % (N = 312) as a light skin tone, and 94.8 % (N = 309) as under 55 years of age. For paramedics and police the gender distribution was a statistically significant variation from that of actual Australian workforce data (all p < 0.001). Among the images of individual paramedics and ambulance officers (N = 32), DALL-E 3 depicted 100 % as male, 100 % as Caucasian and 100 % with light skin tone.
Conclusion
Gender and ethnicity bias is a significant limitation for text-to-image generative AI using DALL-E 3 among Australian first responders. Generated images have a disproportionately high misrepresentation of males, Caucasians and light skin tones that are not representative of the diversity of paramedics in Australia today.}
}
@article{COHEN2025111646,
title = {Generative artificial intelligence and academic writing: friend or foe?},
journal = {Journal of Clinical Epidemiology},
volume = {179},
pages = {111646},
year = {2025},
issn = {0895-4356},
doi = {https://doi.org/10.1016/j.jclinepi.2024.111646},
url = {https://www.sciencedirect.com/science/article/pii/S0895435624004025},
author = {Jérémie F. Cohen and David Moher},
keywords = {Artificial Intelligence, Large language models, Medical writing, Publication ethics, Research ethics, Research integrity},
abstract = {This viewpoint examines the use of generative AI models in medical writing, discusses the opportunities and threats they represent, and highlights avenues for improvement and future research.}
}
@article{SOLAIMAN2024102028,
title = {Generative artificial intelligence (GenAI) and decision-making: Legal & ethical hurdles for implementation in mental health},
journal = {International Journal of Law and Psychiatry},
volume = {97},
pages = {102028},
year = {2024},
issn = {0160-2527},
doi = {https://doi.org/10.1016/j.ijlp.2024.102028},
url = {https://www.sciencedirect.com/science/article/pii/S0160252724000773},
author = {Barry Solaiman},
keywords = {Generative artificial intelligence (GenAI), Mental health, Psychiatry, Ethics, Law, Healthcare},
abstract = {This article argues that significant risks are being taken with using GenAI in mental health that should be assessed urgently. It recommends that guidelines for using generative artificial intelligence (GenAI) in mental health care must be established promptly. Currently, clinicians using chatbots without appropriate approval risk undermining legal protections for patients. This could harm the patient and undermine the standards of the profession, undermining trust in an area where human involvement in decision-making is critical. To explore these concerns, this paper is divided into three parts. First, it examines the needs of patients in mental health. Second, it explores the potential benefits of GenAI in mental health and highlights the risks of its use as it pertains to patient needs. Third, it notes the ethical and legal concerns around data use and medical liability that require careful attention. The impact of the European Union's (EU) Artificial Intelligence Act (AI-Act) is also considered. It will be seen that these laws are insufficient in the context of mental health. As such, the paper recommends that guidelines should be developed to help resolve the existing legal gaps until codified rules are established.}
}
@article{HIROSAWA2025,
title = {Utility of Generative Artificial Intelligence for Japanese Medical Interview Training: Randomized Crossover Pilot Study},
journal = {JMIR Medical Education},
volume = {11},
year = {2025},
issn = {2369-3762},
doi = {https://doi.org/10.2196/77332},
url = {https://www.sciencedirect.com/science/article/pii/S2369376225000996},
author = {Takanobu Hirosawa and Masashi Yokose and Tetsu Sakamoto and Yukinori Harada and Kazuki Tokumasu and Kazuya Mizuta and Taro Shimizu},
keywords = {artificial intelligence, generative artificial intelligence, medical interview training, mock patient, simulation education},
abstract = {Background
The medical interview remains a cornerstone of clinical training. There is growing interest in applying generative artificial intelligence (AI) in medical education, including medical interview training. However, its utility in culturally and linguistically specific contexts, including Japanese, remains underexplored. This study investigated the utility of generative AI for Japanese medical interview training.
Objective
This pilot study aimed to evaluate the utility of generative AI as a tool for medical interview training by comparing its performance with that of traditional face-to-face training methods using a simulated patient.
Methods
We conducted a randomized crossover pilot study involving 20 postgraduate year 1‐2 physicians from a university hospital. Participants were randomly allocated into 2 groups. Group A began with an AI-based station on a case involving abdominal pain, followed by a traditional station with a standardized patient presenting chest pain. Group B followed the reverse order, starting with the traditional station for abdominal pain and subsequently within the AI-based station for the chest pain scenario. In the AI-based stations, participants interacted with a GPT-configured platform that simulated patient behaviors. GPTs are customizable versions of ChatGPT adapted for specific purposes. The traditional stations involved face-to-face interviews with a simulated patient. Both groups used identical, standardized case scenarios to ensure uniformity. Two independent evaluators, blinded to the study conditions, assessed participants’ performances using 6 defined metrics: patient care and communication, history taking, physical examination, accuracy and clarity of transcription, clinical reasoning, and patient management. A 6-point Likert scale was used for scoring. The discrepancy between the evaluators was resolved through discussion. To ensure cultural and linguistic authenticity, all interviews and evaluations were conducted in Japanese.
Results
AI-based stations scored lower across most categories, particularly in patient care and communication, than traditional stations (4.48 vs 4.95; P=.009). However, AI-based stations demonstrated comparable performance in clinical reasoning, with a nonsignificant difference (4.43 vs 4.85; P=.10).
Conclusions
The comparable performance of generative AI in clinical reasoning highlights its potential as a complementary tool in medical interview training. One of its main advantages lies in enabling self-learning, allowing trainees to independently practice interviews without the need for simulated patients. Nonetheless, the lower scores in patient care and communication underline the importance of maintaining traditional methods that capture the nuances of human interaction. These findings support the adoption of hybrid training models that combine generative AI with conventional approaches to enhance the overall effectiveness of medical interview training in Japan.
Trial Registration
UMIN-CTR UMIN000053747; https://center6.umin.ac.jp/cgi-open-bin/ctr_e/ctr_view.cgi?recptno=R000061336}
}
@article{REASON2025,
title = {The “Artificial Intelligence Statistician”: Utilizing Generative Artificial Intelligence to Select an Appropriate Model and Execute Network Meta-Analyses},
journal = {Value in Health},
year = {2025},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2025.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1098301525025112},
author = {Tim Reason and Yunchou Wu and Cheryl Jones and Emma Benbow and Kasper Johannesen and Bill Malcolm},
keywords = {automated analysis, health technology assessment (HTA), joint clinical assessments (JCAs), large language models (LLMs), network meta-analysis (NMA)},
abstract = {Objectives
This exploratory study aimed to develop a large language model (LLM)-based process to automate components of network meta-analysis (NMA), including model selection, analysis, output evaluation, and results interpretation. Automating these tasks with LLMs can enhance efficiency, consistency, and scalability in health economics and outcomes research, while ensuring that analyses adhere to established guidelines required by health technology assessment agencies. Improvements in efficiency and scalability may potentially become relevant as the European Union Health Technology Assessment Regulation comes into force, given anticipated analysis requirements and timelines.
Methods
Using Claude 3.5 Sonnet (V2), a process was designed to automate statistical model selection, NMA output evaluation, and results interpretation based on an “analysis-ready” data set. Validation was assessed by replicating examples from the National Institute for Health and Care Excellence Technical Support Document (TSD2), replicating results of non-Decision Support Unit-published NMAs, and generating comprehensive outputs (eg, heterogeneity, inconsistency, and convergence).
Results
The automated LLM-based process produced accurate results. Compared with TSD2 examples, differences were minimal, within expectations (given differences in sampling frameworks used), and comparable to those observed between estimates produced by the R vignettes against TSD2. Similar consistency was noted for non-Decision Support Unit-published NMA examples. Additionally, the LLM process generated and interpreted comprehensive NMA outputs.
Conclusions
This exploratory study demonstrates the feasibility of LLMs to automate key components of NMAs, determining the requisite NMA framework based only on input data. Further exploring these capabilities could clarify their role in streamlining NMA workflows.}
}
@article{RASHIDI2025100687,
title = {Generative Artificial Intelligence in Pathology and Medicine: A Deeper Dive},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100687},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2024.100687},
url = {https://www.sciencedirect.com/science/article/pii/S0893395224002679},
author = {Hooman H. Rashidi and Joshua Pantanowitz and Alireza Chamanzar and Brandon Fennell and Yanshan Wang and Rama R. Gullapalli and Ahmad Tafti and Mustafa Deebajah and Samer Albahra and Eric Glassy and Matthew G. Hanna and Liron Pantanowitz},
keywords = {ChatGPT, diffusion, generative adversarial network, generative artificial intelligence, generative pretrained transformer, multiagent},
abstract = {This review article builds upon the introductory piece in our 7-part series, delving deeper into the transformative potential of generative artificial intelligence (Gen AI) in pathology and medicine. The article explores the applications of Gen AI models in pathology and medicine, including the use of custom chatbots for diagnostic report generation, synthetic image synthesis for training new models, data set augmentation, hypothetical scenario generation for educational purposes, and the use of multimodal along with multiagent models. This article also provides an overview of the common categories within Gen AI models, discussing open-source and closed-source models, as well as specific examples of popular models such as GPT-4, Llama, Mistral, DALL-E, Stable Diffusion, and their associated frameworks (eg, transformers, generative adversarial networks, diffusion-based neural networks), along with their limitations and challenges, especially within the medical domain. We also review common libraries and tools that are currently deemed necessary to build and integrate such models. Finally, we look to the future, discussing the potential impact of Gen AI on health care, including benefits, challenges, and concerns related to privacy, bias, ethics, application programming interface costs, and security measures.}
}
@article{KHANIFAR2025100069,
title = {Generative artificial intelligence in soil science},
journal = {Soil Advances},
volume = {4},
pages = {100069},
year = {2025},
issn = {2950-2896},
doi = {https://doi.org/10.1016/j.soilad.2025.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2950289625000375},
author = {Javad Khanifar}
}
@article{DUAN2025103719,
title = {The transformative roles of generative artificial intelligence in vision techniques for structural health monitoring: A state-of-the-art review},
journal = {Advanced Engineering Informatics},
volume = {68},
pages = {103719},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103719},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625006123},
author = {Shundi Duan and Xiao Tan and Pengwei Guo and Yurong Guo and Yi Bao},
keywords = {Generative artificial intelligence, Structural health monitoring, Image restoration, Data augmentation, Multi-modal generative AI, Large language model},
abstract = {As urbanization accelerates, aging infrastructure demands more advanced inspection methods for structural health monitoring. The growing integration of artificial intelligence (AI) and computer vision technologies has significantly enhanced damage detection accuracy while simultaneously reducing inspection time and operational costs. Despite these advantages, the adoption of AI-based technologies in infrastructure maintenance remains limited due to challenges related to data. One major issue is the lack of comprehensive, task-specific annotated datasets. Another is the poor quality of images captured by drones or mobile devices, which are often affected by noise, blurring, and inconsistent lighting. Although recent advances in generative AI offer promising support for structural health monitoring, it remains unclear which models are best suited for specific tasks. This study examines the use of generative AI in structural health monitoring, focusing on key challenges such as limited datasets and low-quality image restoration. The review covers a range of generative AI technologies, outlining their principles, strengths, limitations, and representative applications to support the selection of appropriate tools for specific tasks. Generative AI models enable accurate image segmentation and structural anomaly detection using limited training data. The paper also explores new opportunities for integrating multi-modal generative AI to enhance human–computer interaction in support of structural health monitoring. A framework is proposed to streamline the use of generative AI technologies for data augmentation, image restoration, damage inspection, and human–computer interaction in structural health monitoring.}
}
@article{LI2025112349,
title = {Generative artificial intelligence-based framework for bridging lifecycle gaps in semiconductor HVAC systems},
journal = {Journal of Building Engineering},
volume = {105},
pages = {112349},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112349},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225005868},
author = {Yanlin Li and Chi-Yun Liu and Hsiao-Ping Ni and Fermodelie Paul and Wai Oswald Chong and Jui-Sheng Chou},
keywords = {HVAC system performance, Semiconductor manufacturing facility, HVAC equipment degradation prediction, AI-Driven models, Advanced building systems},
abstract = {The production of modern semiconductor chips is susceptible to variations in air temperature, humidity, and quality, mainly as chip dimensions shrink to smaller than atmospheric dust particles. Heating, ventilation, and air-conditioning (HVAC) systems are critical in this context, given their role in stabilizing these environmental factors. Gaps in Design and Construction (D&C) can critically undermine the reliability, performance, quality, and lifespan of HVAC systems during their Operation and Maintenance (O&M) stages. Current studies illustrate how existing models predicted performance degradation and how the Design, Construction, Operations, and Maintenance (DCOM) gap arises. Despite the substantial implications for Semiconductor Manufacturing Facilities (SMFs), research on HVAC performance degradation remains limited, particularly in capturing and quantifying degradation-related patterns. Compared to other types of buildings, the renovation and transformation of high-end manufacturing facilities are more frequent, and customized design and equipment also lead to model application issues, such as data limitations and incompatibility. This paper aims to propose a novel HVAC system degradation prediction model utilizing Generative Adversarial Networks (GAN) and Informer algorithms based on the building characteristics and operation mode of semiconductor facilities to overcome the limitations of data scarcity and long-term prediction, evaluate the comprehensive impact of the gap between D&C and O&M stages on HVAC systems. By integrating data augmentation, this model reduces data dependency and can handle incomplete, inconsistent, or discrete data for early prediction in operation, bridging the gap between D&C and O&M stages, and improving the overall efficiency and effectiveness of facility operation and maintenance. In addition to SMFs, the proposed model exhibits considerable application potential in other high-precision building types due to the structural variability.}
}
@article{FLEURENCE2025175,
title = {Generative Artificial Intelligence for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations: An ISPOR Working Group Report},
journal = {Value in Health},
volume = {28},
number = {2},
pages = {175-183},
year = {2025},
issn = {1098-3015},
doi = {https://doi.org/10.1016/j.jval.2024.10.3846},
url = {https://www.sciencedirect.com/science/article/pii/S1098301524067548},
author = {Rachael L. Fleurence and Jiang Bian and Xiaoyan Wang and Hua Xu and Dalia Dawoud and Mitchell Higashi and Jagpreet Chhatwal},
keywords = {artificial intelligence, economic modeling methods, generative AI, large language models, real world evidence, systematic reviews},
abstract = {Objectives
To provide an introduction to the uses of generative artificial intelligence (AI) and foundation models, including large language models, in the field of health technology assessment (HTA).
Methods
We reviewed applications of generative AI in 3 areas: systematic literature reviews, real-world evidence, and health economic modeling.
Results
(1) Literature reviews: generative AI has the potential to assist in automating aspects of systematic literature reviews by proposing search terms, screening abstracts, extracting data, and generating code for meta-analyses; (2) real-world evidence: generative AI can facilitate automating processes and analyze large collections of real-world data, including unstructured clinical notes and imaging; (3) health economic modeling: generative AI can aid in the development of health economic models, from conceptualization to validation. Limitations in the use of foundation models and large language models include challenges surrounding their scientific rigor and reliability, the potential for bias, implications for equity, as well as nontrivial concerns regarding adherence to regulatory and ethical standards, particularly in terms of data privacy and security. Additionally, we survey the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools.
Conclusions
Although generative AI technology holds promise with respect to HTA applications, it is still undergoing rapid developments and improvements. Continued careful evaluation of their applications to HTA is required. Both developers and users of research incorporating these tools, should familiarize themselves with their current capabilities and limitations.}
}
@article{TADOKORO2025663,
title = {On the effective co-creation of CAD models by leveraging generative artificial intelligence},
journal = {Procedia CIRP},
volume = {134},
pages = {663-668},
year = {2025},
note = {58th CIRP Conference on Manufacturing Systems 2025},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2025.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125005608},
author = {Fuwa Tadokoro and Angkush Kumar Ghosh and Sharifu Ura},
keywords = {Computer-Aided Design, Generative Artificial Intelligence, Co-creation, Prompt Tuning, Modular Decomposition, Information, Complexity},
abstract = {Collaboration between humans and generative artificial intelligence (GenAI) can result in effective problem-solving methods for smart manufacturing. From this point of view, this study presents how to solve computer-aided design (CAD) problems using GenAI-human collaboration, where a GenAI tool generates structured code (syntax) for CAD while a designer articulates the design intent (semantics). The focus is to see how the information type, model complexity, and prompt structure collectively affect the collaboration. A set of case studies demonstrate that a modular modeling approach with low-level information improves prompt efficiency and modeling accuracy, especially in complex scenarios. This strategy also enables novice and expert users to collaborate with GenAI in solving challenging real-world CAD tasks effectively. The findings support the development of advanced human-AI co-creation systems and encourage future research in areas such as reverse engineering, multi-part assemblies, and collaborative CAD workflows coupled with complex design constraints.}
}
@article{CHENG2025101497,
title = {Online reviews generated by generative artificial intelligence versus human: A study of perceived differences and user adoption behavior},
journal = {Electronic Commerce Research and Applications},
volume = {71},
pages = {101497},
year = {2025},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2025.101497},
url = {https://www.sciencedirect.com/science/article/pii/S1567422325000225},
author = {Xusen Cheng and Ang Zeng and Bo Yang and Yu Liu and Xiaoping Zhang},
keywords = {generative artificial intelligence (GAI), GAI-generated reviews, human-generated reviews, willingness to use GAI},
abstract = {Companies in various industries are attempting to integrate Generative Artificial Intelligence (GAI) into their existing businesses. In the e-commerce domain, GAI has shown tremendous potential in generating online reviews. However, existing literature has paid less attention to how consumers respond to GAI-generated reviews versus human-generated reviews. Moreover, little research has explored whether and why consumers are willing to use GAI to generate online reviews. By conducting two experiments, this study investigates how consumers respond differently to GAI-generated reviews versus human-generated reviews and identifies potential factors that influence consumers’ willingness to use GAI to generate reviews. Findings indicate that although there is no significant difference in consumers’ perceptions between human-generated and GAI-generated reviews in terms of review credibility, review richness, and review usefulness, only half of the participants are willing to use GAI to generate reviews. Further analysis results suggest that individuals who consider GAI unethical tend to avoid using GAI. Those with high personal innovativeness are more willing to use GAI to generate online reviews. Our findings deepen the understanding of consumer attitudes toward GAI-generated reviews and provide implications for the deployment of GAI in the online review system.}
}
@article{WOBST2025115571,
title = {Avoiding algorithm errors in textual analysis: A guide to selecting software, and a research agenda toward generative artificial intelligence},
journal = {Journal of Business Research},
volume = {199},
pages = {115571},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115571},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325003947},
author = {Janice Wobst and Rainer Lueg},
keywords = {Generative AI, Large language models, Textual analysis, Software selection, Algorithm error, Validity, Reliability, Value-based management},
abstract = {The use of textual analysis is expanding in organizational research, yet software packages vary in their compatibility with complex constructs. This study helps researchers select suitable tools by focusing on phrase-based dictionary methods. We empirically evaluate four software packages—LIWC, DICTION, CAT Scanner, and a custom Python tool—using the complex construct of value-based management as a test case. The analysis shows that software from the same methodological family produces highly consistent results, while popular but mismatched tools yield significant errors such as miscounted phrases. Based on this, we develop a structured selection guideline that links construct features with software capabilities. The framework enhances construct validity, supports methodological transparency, and is applicable across disciplines. Finally, we position the approach as a bridge to AI-enabled textual analysis, including prompt-based workflows, reinforcing the continued need for theory-grounded construct design.}
}
@article{DEEB20241724,
title = {The emerging role of generative artificial intelligence in transplant medicine},
journal = {American Journal of Transplantation},
volume = {24},
number = {10},
pages = {1724-1730},
year = {2024},
issn = {1600-6135},
doi = {https://doi.org/10.1016/j.ajt.2024.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S1600613524003824},
author = {Maya Deeb and Anirudh Gangadhar and Madhumitha Rabindranath and Khyathi Rao and Michael Brudno and Aman Sidhu and Bo Wang and Mamatha Bhat},
keywords = {artificial intelligence, deep learning, generative adversarial networks, large language models, machine learning, natural language processing, variational autoencoders},
abstract = {Generative artificial intelligence (AI), a subset of machine learning that creates new content based on training data, has witnessed tremendous advances in recent years. Practical applications have been identified in health care in general, and there is significant opportunity in transplant medicine for generative AI to simplify tasks in research, medical education, and clinical practice. In addition, patients stand to benefit from patient education that is more readily provided by generative AI applications. This review aims to catalyze the development and adoption of generative AI in transplantation by introducing basic AI and generative AI concepts to the transplant clinician and summarizing its current and potential applications within the field. We provide an overview of applications to the clinician, researcher, educator, and patient. We also highlight the challenges involved in bringing these applications to the bedside and need for ongoing refinement of generative AI applications to sustainably augment the transplantation field.}
}
@article{PARK2024428,
title = {Generative artificial intelligence in nursing: A scoping review},
journal = {Collegian},
volume = {31},
number = {6},
pages = {428-436},
year = {2024},
issn = {1322-7696},
doi = {https://doi.org/10.1016/j.colegn.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1322769624000696},
author = {Ga Eun Park and Hyeryeon Kim and U Ri Go},
keywords = {Generative artificial intelligence, Artificial intelligence, ChatGPT, Nursing, Nurses, Healthcare, Machine learning},
abstract = {ABSTRACT
Background
Generative artificial intelligence (AI) is rapidly transforming multiple sectors, with significant potential to revolutionise nursing through advancements in education, practice, and research. However, the application of generative AI in nursing remains underexplored, highlighting the need for a comprehensive review of its current impact and future implications.
Aim
To investigate the current state and implications of generative AI in nursing education, practice, and research.
Methods
This scoping review was conducted following the methodological frameworks of Arksey and O’Malley, refined by Levac and colleagues. The databases searched for articles published between January 2020 and April 2024 included PubMed, Embase, CINAHL, PsycINFO, Cochrane Library, and IEEE Xplore.
Findings
A total of 4858 articles were identified, with 23 included in this review. Most of the selected studies were published in 2024 (n = 19/23), primarily conducted in the United States (n = 8/23), and largely consisted of quantitative descriptive studies (n = 14/22). ChatGPT was the most frequently used tool, appearing in 95.7% of the studies (n = 22/23). The articles addressed various nursing domains, including nursing education (n = 12/23), practice (n = 10/23), and research (n = 1/23). Both the benefits and concerns associated with this technology were identified.
Discussion
Generative AI shows great promise for transforming nursing education, practice, and research; however, its integration is still in the early stages.
Conclusion
To fully leverage the benefits of generative AI, nursing professionals must address the challenges associated with AI and lead its ethical adoption. Rigorous research and proactive leadership are crucial to realising the potential of generative AI in nursing.}
}
@article{ALBUSAIDI2024100630,
title = {Redefining boundaries in innovation and knowledge domains: Investigating the impact of generative artificial intelligence on copyright and intellectual property rights},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100630},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100630},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001690},
author = {Adil S. Al-Busaidi and Raghu Raman and Laurie Hughes and Mousa Ahmed Albashrawi and Tegwen Malik and Yogesh K. Dwivedi and Thuraiya {Al- Alawi} and Mohammed AlRizeiqi and Gareth Davies and Mark Fenwick and Parul Gupta and Shashikala Gurpur and Apeksha Hooda and Paulius Jurcys and Daryl Lim and Nicola Lucchi and Tanvi Misra and Ramakrishnan Raman and Anuragini Shirish and Paul Walton},
keywords = {ChatGPT, Generative artificial intelligence, GenAI, Generative scholar, Innovation, Intellectual property (IP) Risks, Large language models (LLMs), Misuse case analysis, Personality rights},
abstract = {The rapid integration of generative AI (GenAI) into industries and society has prompted a re-evaluation of copyright and intellectual property rights (IPR) frameworks. GenAI's ability to produce original content using data from human-created sources raises critical ethical and legal concerns. Current copyright and IPR frameworks, designed around human authorship, are insufficient to address these challenges. This study, using a multi-perspective approach, explores GenAI's disruptive potential in replicating or transforming copyrighted materials, challenging established IPR norms. Findings highlight gaps in legislation and the opacity of GenAI platforms. To address these issues, this study presents a Dynamic Ethical Framework linked to a future global fair use policy, aiming to guide responsible GenAI development and use. By incorporating insights from domain experts, this study contextualizes emerging challenges and potential solutions within broader societal and technological trends. That said, this study calls for international collaboration and further research to reform IPR related laws and frameworks, ensuring they remain relevant and equitable in a GenAI-driven era.}
}
@article{LIU20251202,
title = {Environmental Assessment and Improvement of Factory Building Designs based on Generative Artificial Intelligence},
journal = {Procedia CIRP},
volume = {135},
pages = {1202-1207},
year = {2025},
note = {32nd CIRP Conference on Life Cycle Engineering (LCE2025)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.12.119},
url = {https://www.sciencedirect.com/science/article/pii/S2212827125004184},
author = {Shengyu Liu and Sipke Hoekstra and Sebastian Thiede},
keywords = {Generative Artificial Intelligence, Life Cycle Assessment, Factory},
abstract = {The paper explores an innovative approach to evaluate the environmental impact of factory buildings at early design stages. Generative design, a cutting-edge computational technique, is employed to generate multiple factory building design alternatives based on user and case specific boundary conditions, e.g. related to material flow and space restrictions. This paper aims to integrate generative design principles with environmental assessment metrics to improve factory buildings for minimal environmental footprint, e.g. driven through energy demand. Thus, a framework that combines the generative factory design approach with key environmental assessment parameters is introduced. The effectiveness of generative design in enhancing the environmental performance of factory buildings is demonstrated with a case study. A comparative analysis of different designs highlights main influencing factors, as well as trade-offs and synergies between different manufacturing system performances and environmental oriented objectives. With that, the paper underlines the value of generative design as a transformative tool in sustainable factory design and provides actionable insights for architects, engineers, and policymakers aiming to develop greener industrial facilities.}
}
@article{LIEBSCHER2025e106,
title = {Testing the Implementation and Acceptance of Generative Artificial Intelligence to Augment Vascular Surgery Journal Club},
journal = {Journal of Vascular Surgery},
volume = {82},
number = {4},
pages = {e106},
year = {2025},
issn = {0741-5214},
doi = {https://doi.org/10.1016/j.jvs.2025.06.096},
url = {https://www.sciencedirect.com/science/article/pii/S0741521425014764},
author = {Sean Liebscher and Rhea Puthumana and Mead Ferris and Daniel Bertges}
}
@article{TRINDADE2025101104,
title = {Teaching mathematical concepts in management with generative artificial intelligence: The power of human oversight in AI-driven learning},
journal = {The International Journal of Management Education},
volume = {23},
number = {2},
pages = {101104},
year = {2025},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101104},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001757},
author = {Maria A.M. Trindade and Gihan S. Edirisinghe and Lan Luo},
keywords = {Generative artificial intelligence in education, Generative AI-Driven learning, Mathematics in management education, Operations management, Economic order quantity, Generative AI in management education},
abstract = {This study demonstrates a successful use of Generative Artificial Intelligence (AI) in teaching mathematical material to management students. We herein introduce the EOQ World Tour game, which substantially improves understanding of inventory-related concepts and long-term knowledge retention compared with traditional methods. Generative AI is revolutionizing management education, by offering innovative methods for teaching and learning. The integration of AI into quantitative business disciplines through novel learning mechanisms provides significant benefits, including enhanced data analysis, improved decision-making models, and sophisticated simulations for hands-on experience. This study introduces the EOQ World Tour game, specifically designed to teach the Economic Order Quantity concept in Operations Management. The game addresses challenges in integrating Generative AI into mathematics in management education by combining human oversight and instructor control through three innovative features: (1) a Generative AI-based simulation, (2) a macropowered Excel worksheet for validating the calculations of an AI chatbot, and (3) a Google Sheets dashboard for centralizing team-generated AI data for postgame analysis. Our study included 41 students divided into experimental and control groups. Pretest results indicated no significant differences in baseline knowledge. However, the post-test results showed that the experimental group achieved a better understanding of inventory-related concepts and practical applications, along with higher engagement, excitement, confidence, and long-term knowledge retention.}
}
@article{KONG2024100328,
title = {Examining teachers’ behavioural intention of using generative artificial intelligence tools for teaching and learning based on the extended technology acceptance model},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100328},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100328},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001310},
author = {Siu Cheung Kong and Yin Yang and Chunyu Hou},
keywords = {Artificial intelligence literacy, Generative artificial intelligence tools, Teacher development, Teaching and learning, Extended technology acceptance model},
abstract = {The rapid development of generative artificial intelligence (GenAI) tools has given rise to a growing discussion of the potential challenges and benefits that the use of these technologies may present in the field of education. This study examines the acceptance of the use of GenAI tools for teaching and learning among primary and secondary school teachers in Hong Kong. It uses an extension of the technology acceptance model (TAM) with a modified framework that incorporates two key factors: self-efficacy and subjective norm. Data were collected from a sample of 367 primary and secondary school teachers in Hong Kong using questionnaires containing items for six constructs: self-efficacy, perceived usefulness, perceived ease of use, attitude towards using, subjective norm, and behavioural intention. The results show that fostering teachers' self-efficacy, perceived usefulness, and attitude is essential for successfully increasing their behavioural intention to use GenAI tools. Subjective norm was also found to influence teachers' behavioural intention. To enhance teachers' effective use of GenAI for teaching, teacher development programmes should focus on equipping teachers with comprehensive conceptual knowledge and skills and an understanding of the application of these tools to teaching and learning. Policy support to create a conducive environment for the use of GenAI in teaching and learning would also be beneficial. The study has theoretical implications in its extension of the TAM model as well as implications for enhancing teachers’ AI literacy and developing pedagogies for the meaningful use of GenAI tools for teaching and learning in K–12 settings.}
}
@article{RASHIDI2025100663,
title = {Statistics of Generative Artificial Intelligence and Nongenerative Predictive Analytics Machine Learning in Medicine},
journal = {Modern Pathology},
volume = {38},
number = {3},
pages = {100663},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2024.100663},
url = {https://www.sciencedirect.com/science/article/pii/S0893395224002436},
author = {Hooman H. Rashidi and Bo Hu and Joshua Pantanowitz and Nam Tran and Silvia Liu and Alireza Chamanzar and Mert Gur and Chung-Chou H. Chang and Yanshan Wang and Ahmad Tafti and Liron Pantanowitz and Matthew G. Hanna},
keywords = {BiLingual Evaluation Understudy, accuracy, precision, F1 score, receiver operating characteristic area under the curve, perplexity},
abstract = {The rapidly evolving landscape of artificial intelligence (AI) and machine learning (ML) in medicine has prompted medical professionals to increasingly familiarize themselves with related topics. This also demands grasping the underlying statistical principles that govern their design, validation, and reproducibility. Uniquely, the practice of pathology and medicine produces vast amount of data that can be exploited by AI/ML. The emergence of generative AI, especially in the area of large language models and multimodal frameworks, represents approaches that are starting to transform medicine. Fundamentally, generative and traditional (eg, nongenerative predictive analytics) ML techniques rely on certain common statistical measures to function. However, unique to generative AI are metrics such as, but not limited to, perplexity and BiLingual Evaluation Understudy score that provide a means to determine the quality of generated samples that are typically unfamiliar to most medical practitioners. In contrast, nongenerative predictive analytics ML often uses more familiar metrics tailored to specific tasks as seen in the typical classification (ie, confusion metrics measures, such as accuracy, sensitivity, F1 score, and receiver operating characteristic area under the curve) or regression studies (ie, root mean square error and R2). To this end, the goal of this review article (as part 4 of our AI review series) is to provide an overview and a comparative measure of statistical measures and methodologies used in both generative AI and traditional (ie, nongenerative predictive analytics) ML fields along with their strengths and known limitations. By understanding their similarities and differences along with their respective applications, we will become better stewards of this transformative space, which ultimately enables us to better address our current and future needs and challenges in a more responsible and scientifically sound manner.}
}
@article{WISLOCKI2025,
title = {Comparing Generative Artificial Intelligence and Mental Health Professionals for Clinical Decision-Making With Trauma-Exposed Populations: Vignette-Based Experimental Study},
journal = {JMIR Mental Health},
volume = {12},
year = {2025},
issn = {2368-7959},
doi = {https://doi.org/10.2196/80801},
url = {https://www.sciencedirect.com/science/article/pii/S2368795925001040},
author = {Katherine E Wislocki and Sabahat Sami and Gahl Liberzon and Alyson K Zalta},
keywords = {generative artificial intelligence, trauma, mental health professionals, diagnosis, treatment},
abstract = {Background
Trauma exposure is highly prevalent and associated with various health issues. However, health care professionals can exhibit trauma-related diagnostic overshadowing bias, leading to misdiagnosis and inadequate treatment of trauma-exposed populations. Generative artificial intelligence (GAI) models are increasingly used in health care contexts. No research has examined whether GAI demonstrates this bias in decision-making and how rates of this bias may compare to mental health professionals (MHPs).
Objective
This study aimed to assess trauma-related diagnostic overshadowing among frontier GAI models and compare evidence of trauma-related diagnostic overshadowing between frontier GAI models and MHPs.
Methods
MHPs (N=232; mean [SD] age 43.7 [15.95] years) completed an experimental paradigm consisting of 2 vignettes describing adults presenting with obsessive-compulsive symptoms or substance abuse symptoms. One vignette included a trauma exposure history (ie, sexual trauma or physical trauma), and one vignette did not include a trauma exposure history. Participants answered questions about their preferences for diagnosis and treatment options for clients within the vignettes. GAI models (eg, Gemini 1.5 Flash, ChatGPT-4o mini, Claude Sonnet, and Meta Llama 3) completed the same experimental paradigm, with each block being reviewed by each GAI model 20 times. Mann-Whitney U tests and chi-square analyses were used to assess diagnostic and treatment decision-making across vignette factors and respondents.
Results
GAI models, similar to MHPs, demonstrated some evidence of trauma-related diagnostic overshadowing bias, particularly in Likert-based ratings of posttraumatic stress disorder diagnosis and treatment when sexual trauma was present (P<.001). However, GAI models generally exhibited significantly less bias than MHPs across both Likert and forced-choice clinical decision tasks. Compared to MHPs, GAI models assigned higher ratings for the target diagnosis and treatment in obsessive-compulsive disorder vignettes (rb=0.43‐0.63; P<.001) and for the target treatment in substance use disorder vignettes (rb=0.57; P<.001) when trauma was present. In forced-choice tasks, GAI models were significantly more accurate than MHPs in selecting the correct diagnosis and treatment for obsessive-compulsive disorder vignettes (χ²1=48.84‐61.07; P<.001) and for substance use disorder vignettes involving sexual trauma (χ²1=15.17‐101.61; P<.001).
Conclusions
GAI models demonstrate some evidence of trauma-related diagnostic overshadowing bias, yet the degree of bias varied by task and model. Moreover, GAI models generally demonstrated less bias than MHPs in this experimental paradigm. These findings highlight the importance of understanding GAI biases in mental health care. More research into bias reduction strategies and responsible implementation of GAI models in mental health care is needed.}
}
@article{JANG2024102175,
title = {When, What, and how should generative artificial intelligence explain to Users?},
journal = {Telematics and Informatics},
volume = {93},
pages = {102175},
year = {2024},
issn = {0736-5853},
doi = {https://doi.org/10.1016/j.tele.2024.102175},
url = {https://www.sciencedirect.com/science/article/pii/S0736585324000790},
author = {Soobin Jang and Haeyoon Lee and Yujin Kim and Daeho Lee and Jungwoo Shin and Jungwoo Nam},
keywords = {Generative AI, Conversational user interface, Explainable AI, Conjoint analysis},
abstract = {With the commercialization of ChatGPT, generative artificial intelligence (AI) has been applied almost everywhere in our lives. However, even though generative AI has become a daily technology that anyone can use, most non-majors need to know the process and reason for the results because it can be misused due to lack of sufficient knowledge and misunderstanding. Therefore, this study investigated users’ preferences for when, what, and how generative AI should provide explanations about the process of generating and the reasoning behind the results, using conjoint method and mixed logit analysis. The results show that users are most sensitive to the timing of providing eXplainable AI (XAI), and that users want additional information only when they ask for explanations during the process of using generative AI. The results of this study will help shape the XAI design of future generative AI from a user perspective and improve usability.}
}
@article{ICHIKAWA2025,
title = {Generative Artificial Intelligence in Medical Education—Policies and Training at US Osteopathic Medical Schools: Descriptive Cross-Sectional Survey},
journal = {JMIR Medical Education},
volume = {11},
year = {2025},
issn = {2369-3762},
doi = {https://doi.org/10.2196/58766},
url = {https://www.sciencedirect.com/science/article/pii/S2369376225000236},
author = {Tsunagu Ichikawa and Elizabeth Olsen and Arathi Vinod and Noah Glenn and Karim Hanna and Gregg C Lund and Stacey Pierce-Talsma},
keywords = {artificial intelligence, medical education, faculty development, policy, AI, training, United States, school, university, college, institution, osteopathic, osteopathy, curriculum, student, faculty, administrator, survey, cross-sectional},
abstract = {Background
Interest has recently increased in generative artificial intelligence (GenAI), a subset of artificial intelligence that can create new content. Although the publicly available GenAI tools are not specifically trained in the medical domain, they have demonstrated proficiency in a wide range of medical assessments. The future integration of GenAI in medicine remains unknown. However, the rapid availability of GenAI with a chat interface and the potential risks and benefits are the focus of great interest. As with any significant medical advancement or change, medical schools must adapt their curricula to equip students with the skills necessary to become successful physicians. Furthermore, medical schools must ensure that faculty members have the skills to harness these new opportunities to increase their effectiveness as educators. How medical schools currently fulfill their responsibilities is unclear. Colleges of Osteopathic Medicine (COMs) in the United States currently train a significant proportion of the total number of medical students. These COMs are in academic settings ranging from large public research universities to small private institutions. Therefore, studying COMs will offer a representative sample of the current GenAI integration in medical education.
Objective
This study aims to describe the policies and training regarding the specific aspect of GenAI in US COMs, targeting students, faculty, and administrators.
Methods
Web-based surveys were sent to deans and Student Government Association (SGA) presidents of the main campuses of fully accredited US COMs. The dean survey included questions regarding current and planned policies and training related to GenAI for students, faculty, and administrators. The SGA president survey included only those questions related to current student policies and training.
Results
Responses were received from 81% (26/32) of COMs surveyed. This included 47% (15/32) of the deans and 50% (16/32) of the SGA presidents (with 5 COMs represented by both the deans and the SGA presidents). Most COMs did not have a policy on the student use of GenAI, as reported by the dean (14/15, 93%) and the SGA president (14/16, 88%). Of the COMs with no policy, 79% (11/14) had no formal plans for policy development. Only 1 COM had training for students, which focused entirely on the ethics of using GenAI. Most COMs had no formal plans to provide mandatory (11/14, 79%) or elective (11/15, 73%) training. No COM had GenAI policies for faculty or administrators. Eighty percent had no formal plans for policy development. Furthermore, 33.3% (5/15) of COMs had faculty or administrator GenAI training. Except for examination question development, there was no training to increase faculty or administrator capabilities and efficiency or to decrease their workload.
Conclusions
The survey revealed that most COMs lack GenAI policies and training for students, faculty, and administrators. The few institutions with policies or training were extremely limited in scope. Most institutions without current training or policies had no formal plans for development. The lack of current policies and training initiatives suggests inadequate preparedness for integrating GenAI into the medical school environment, therefore, relegating the responsibility for ethical guidance and training to the individual COM member.}
}
@article{RIANTO2025104427,
title = {Generative artificial intelligence for fire scenario analysis in complex building design layouts},
journal = {Fire Safety Journal},
volume = {155},
pages = {104427},
year = {2025},
issn = {0379-7112},
doi = {https://doi.org/10.1016/j.firesaf.2025.104427},
url = {https://www.sciencedirect.com/science/article/pii/S0379711225000918},
author = {Shandy Rianto and Yanfu Zeng and Xinyan Huang and Xinzheng Lu},
keywords = {Intelligent design, Fire safety engineering, Generative AI, Building fire simulation, Computational fluid dynamics},
abstract = {Performance-based fire safety design requires thoroughly evaluating building fire scenarios to ensure comprehensive fire safety. However, conventional Computational Fluid Dynamics (CFD) fire simulations are computationally intensive and time-consuming, limiting the number of scenarios that can be practically analyzed. This study addresses these challenges by using generative artificial intelligence (AI) to predict fire scenes in realistic multi-room building layouts, characterized by complex shapes and intricate wall partitions. Three generative AI models for image generation are employed for this purpose: GAN-based pix2pix and pix2pixHD, as well as the diffusion model. These models were trained on an extensive dataset of CFD fire simulations to generate near-ceiling smoke movement and temperature distribution outcomes. When tested on new unseen building layouts, these models demonstrated remarkable accuracy and provided near real-time assessments. The diffusion model achieved the highest accuracy (>94 %) while requiring the more computational time. The high performance of these models highlights the potential of using generative AI to enhance fire safety engineering by enabling faster and more comprehensive fire risk assessments.}
}
@article{MONZON2025,
title = {Leveraging Generative Artificial Intelligence to Improve Motivation and Retrieval in Higher Education Learners},
journal = {JMIR Medical Education},
volume = {11},
year = {2025},
issn = {2369-3762},
doi = {https://doi.org/10.2196/59210},
url = {https://www.sciencedirect.com/science/article/pii/S2369376225000352},
author = {Noahlana Monzon and Franklin Alan Hays},
keywords = {educational technology, retrieval practice, flipped classroom, cognitive engagement, personalized learning, generative artificial intelligence, higher education, university education, learners, instructors, curriculum structure, learning, technologies, innovation, academic misconduct, gamification, self-directed, socio-economic disparities, interactive approach, medical education, chatGPT, machine learning, AI, large language models},
abstract = {Generative artificial intelligence (GenAI) presents novel approaches to enhance motivation, curriculum structure and development, and learning and retrieval processes for both learners and instructors. Though a focus for this emerging technology is academic misconduct, we sought to leverage GenAI in curriculum structure to facilitate educational outcomes. For instructors, GenAI offers new opportunities in course design and management while reducing time requirements to evaluate outcomes and personalizing learner feedback. These include innovative instructional designs such as flipped classrooms and gamification, enriching teaching methodologies with focused and interactive approaches, and team-based exercise development among others. For learners, GenAI offers unprecedented self-directed learning opportunities, improved cognitive engagement, and effective retrieval practices, leading to enhanced autonomy, motivation, and knowledge retention. Though empowering, this evolving landscape has integration challenges and ethical considerations, including accuracy, technological evolution, loss of learner’s voice, and socioeconomic disparities. Our experience demonstrates that the responsible application of GenAI’s in educational settings will revolutionize learning practices, making education more accessible and tailored, producing positive motivational outcomes for both learners and instructors. Thus, we argue that leveraging GenAI in educational settings will improve outcomes with implications extending from primary through higher and continuing education paradigms.}
}
@article{BLEASE2025,
title = {Generative Artificial Intelligence in Primary Care: Qualitative Study of UK General Practitioners’ Views},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/74428},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125010520},
author = {Charlotte Blease and Carolina {Garcia Sanchez} and Cosima Locher and Brian McMillan and Jens Gaab and John Torous},
keywords = {generative AI, general practice, primary care, large language models, education, training, online survey questionnaire, qualitative research., artificial intelligence},
abstract = {Background
The potential for generative artificial intelligence (GenAI) to assist with clinical tasks is the subject of ongoing debate within biomedical informatics and related fields.
Objective
This study aimed to explore general practitioners’ (GPs’) opinions about GenAI on primary care.
Methods
In January 2025, we conducted a web-based survey of 1005 UK GPs’ experiences and opinions of GenAI in clinical practice. This study involved a qualitative inductive descriptive analysis of a written response (“comments”) to an open-ended question in the survey. After analysis, the interpretation of themes was also informed by the technology acceptance model.
Results
Out of 1005 respondents, 611 GPs (61%) provided written comments in response to the free text question, totaling 7990 words. Comments were classified into 3 major themes and 8 subthemes in relation to GenAI in clinical practice. The major themes were (1) unfamiliarity, (2) ambivalence and anxiety, and (3) role in clinical tasks. “Unfamiliarity” encompassed a lack of experience and knowledge, and the need for training on GenAI. “Ambivalence and anxiety” included mixed expectations among GPs in relation to these tools, beliefs about diminished human connection, and skepticism about AI accountability. Finally, commenting on the role of GenAI in clinical tasks, GPs believed it would help with documentation. However, respondents questioned AI’s clinical judgment and raised concerns about operational uncertainty concerning these tools. Female GPs were more likely to leave comments than male GPs, with 53% (324/611) of female GPs providing feedback compared to 41.1% (162/394) who did not. Chi-square tests confirmed this difference ((χ²₂= 14.6, P=.001). In addition, doctors who left comments were significantly more likely to have used GenAI in clinical practice compared with those who did not. Among all respondents, 71.7% (438/611) had not used GenAI. However, noncommenters were even less likely to have used it, with 80.7% (318/394) reporting no use. A chi-square test confirmed this difference (χ²₁=10.0, P=.002).
Conclusions
This study provides timely insights into UK GPs’ perspectives on the role, impact, and limitations of GenAI in primary care. However, the study has limitations. The qualitative data analyzed originates from a self-selected subset of respondents who chose to provide free-text comments, and these participants were more likely to have used GenAI tools in clinical practice. However, the substantial number of comments offers valuable insights into the diverse views held by GPs regarding GenAI. Furthermore, the majority of our respondents reported limited experience and training with these tools; however, many GPs perceived potential benefits of GenAI and ambient AI for documentation. Notably, 2 years after the widespread introduction of GenAI, GPs’ persistent lack of understanding and training remains a critical concern. More extensive qualitative work would provide a more in-depth understanding of GPs’ views.}
}